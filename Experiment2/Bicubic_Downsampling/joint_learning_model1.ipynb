{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS']='2'\n",
    "os.environ['LD_LIBRARY_PATH']=''\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pengyu.zhang/project/superres/ProbSR/Experiment2\n"
     ]
    }
   ],
   "source": [
    "%cd /home/pengyu.zhang/project/superres/ProbSR/Experiment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generation import *\n",
    "from scipy.linalg import sqrtm\n",
    "from downscaling import *\n",
    "from utils import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pengyu.zhang/project/superres/ProbSR/Experiment2/Bicubic_Downsampling\n"
     ]
    }
   ],
   "source": [
    "%cd Bicubic_Downsampling/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langevin & Training Downscale Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upscale By 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_low = 40\n",
    "N_high = 160\n",
    "scale = 4\n",
    "\n",
    "h_low = 1/(N_low-1)\n",
    "x_low = np.arange(0,1.0001,h_low)\n",
    "y_low = np.arange(0,1.0001,h_low)\n",
    "\n",
    "h_high = 1/(N_high-1)\n",
    "x_high = np.arange(0,1.0001,h_high)\n",
    "y_high = np.arange(0,1.0001,h_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_high = create_A(N_high)\n",
    "A_low = create_A(N_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Parameters for prior variance\n",
    "prior_sigma = 0.002\n",
    "ll_sigma = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_high = torch.tensor(A_high).to(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_high = A_high.to_sparse()\n",
    "operator = (A_high.T) * (1/prior_sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataFromH5File4(\"/home/pengyu.zhang/project/superres/ProbSR/Experiment2/data/40_160_low_forcing.h5\")\n",
    "trainset = random.sample(range(0, 1000), 600)\n",
    "testset = [i for i in range(0,1000) if i not in trainset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data():\n",
    "    coefficient = random.sample(trainset,1)[0]\n",
    "    forcing = dataset[coefficient][0]\n",
    "    lr = dataset[coefficient][1]\n",
    "    \n",
    "    return forcing, lr\n",
    "\n",
    "\n",
    "def sample_p_0(x):\n",
    "    # Randomly sampling for initialisation of the Langevin dynamics\n",
    "    # prior = torch.randn(*[batch_size,1,20,20]).to(device)\n",
    "    \n",
    "    # Set the u_low_mean to the initialisation of the Langevin dynamics\n",
    "    # posterior_initial = torch.randn([N_high,N_high]).to(torch.float32)\n",
    "    # posterior_initial = torch.tensor(posterior_initial).to(device).to(torch.float32)\n",
    "    posterior_initial  = F.interpolate(x.reshape(1,1,N_low,N_low),(N_high,N_high),mode=\"bicubic\").reshape(N_high,N_high)\n",
    "    \n",
    "    return posterior_initial\n",
    "\n",
    "    \n",
    "def ula_posterior_preconditioner(z, b_high, x, G):\n",
    "    \"\"\"\n",
    "    Langevin dynamics with preconditioner\n",
    "    \"\"\"\n",
    "    z = z.clone().detach().requires_grad_(True)\n",
    "    sum = 0\n",
    "    for i in range(K):\n",
    "        # Grad log-likelihood\n",
    "        x_hat = G(z.reshape(1,N_high,N_high)).reshape(N_low,N_low)\n",
    "        log_likelihood = (-1/(2*math.pow(ll_sigma, 2)) * torch.matmul((x-x_hat).reshape(1,N_low**2),(x-x_hat).reshape(N_low**2,1)))\n",
    "        grad_ll = torch.autograd.grad(log_likelihood, z)[0]\n",
    "\n",
    "        # Grad prior\n",
    "        difference = torch.spmm(A_high,z.reshape(N_high*N_high,1)) - b_high.reshape(N_high**2,1)\n",
    "        # log_prior = - 0.5 * difference.T @ G_inverse @ difference\n",
    "        # grad_log_prior = torch.autograd.grad(log_prior, z)[0]\n",
    "        grad_log_prior = (- torch.spmm(operator,difference)).reshape(N_high,N_high)\n",
    "        \n",
    "        # Random noise term\n",
    "        W = torch.randn(*[N_high,N_high]).to(device)\n",
    "        # random = torch.matmul(G_sqrt,W.reshape(N_high**2,1)).reshape(N_high,N_high)\n",
    "        \n",
    "        z = z + 0.5 * s ** 2 * grad_log_prior + 0.5 * s ** 2 * grad_ll + s * W\n",
    "        if i >= K-10:\n",
    "            sum += z\n",
    "        \n",
    "    sum /= 10\n",
    "           \n",
    "    return sum.detach() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory already exists\n",
      "2024-08-06 17:50:55,867 : Training for 1000 epoches and learning rate is 0.003\n",
      "Epoch: 1 Loss: tensor(96617.6406, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 2 Loss: tensor(1035.0780, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 3 Loss: tensor(44756.4766, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 4 Loss: tensor(76884.2969, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 5 Loss: tensor(254926.2500, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 6 Loss: tensor(28378.2305, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 7 Loss: tensor(10447.2666, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 8 Loss: tensor(4519.7935, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 9 Loss: tensor(613.0909, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 10 Loss: tensor(1348.8289, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 11 Loss: tensor(833.5715, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 12 Loss: tensor(109.5991, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 13 Loss: tensor(411.8312, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 14 Loss: tensor(487.5277, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 15 Loss: tensor(1496.3713, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 16 Loss: tensor(4285.3730, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 17 Loss: tensor(908.1777, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 18 Loss: tensor(3046.0439, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 19 Loss: tensor(1061.9889, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 20 Loss: tensor(561.5039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 21 Loss: tensor(1919.9478, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 22 Loss: tensor(1374.8564, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 23 Loss: tensor(7841.9121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 24 Loss: tensor(43065.4297, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 25 Loss: tensor(2848.0752, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 26 Loss: tensor(2598.9946, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 27 Loss: tensor(53783., device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 28 Loss: tensor(931477.6875, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 29 Loss: tensor(1590302.2500, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 30 Loss: tensor(38943.0859, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 31 Loss: tensor(265.2379, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 32 Loss: tensor(191.1454, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 33 Loss: tensor(97.1635, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 34 Loss: tensor(652.5396, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 35 Loss: tensor(378.3176, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 36 Loss: tensor(2345.7070, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 37 Loss: tensor(2511.4058, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 38 Loss: tensor(5383.0508, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 39 Loss: tensor(6136.3413, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 40 Loss: tensor(77.6686, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 41 Loss: tensor(510.6321, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 42 Loss: tensor(202.9955, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 43 Loss: tensor(294.2179, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 44 Loss: tensor(3911.5833, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 45 Loss: tensor(73.8349, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 46 Loss: tensor(297.0319, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 47 Loss: tensor(1177.4282, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 48 Loss: tensor(1488.1760, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 49 Loss: tensor(2153.2837, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 50 Loss: tensor(620.7213, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 51 Loss: tensor(2348.2705, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 52 Loss: tensor(11453.2324, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 53 Loss: tensor(125370.4609, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 54 Loss: tensor(9776.4717, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 55 Loss: tensor(8443.5625, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 56 Loss: tensor(317061.2500, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 57 Loss: tensor(141086.4062, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 58 Loss: tensor(1912182.2500, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 59 Loss: tensor(266719.6250, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 60 Loss: tensor(2016451.5000, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 61 Loss: tensor(33478.7461, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 62 Loss: tensor(2966.7185, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 63 Loss: tensor(8727.8115, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 64 Loss: tensor(723.1086, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 65 Loss: tensor(607.2880, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 66 Loss: tensor(399.3093, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 67 Loss: tensor(913.1877, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 68 Loss: tensor(241.5453, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 69 Loss: tensor(2330.9707, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 70 Loss: tensor(2023.4751, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 71 Loss: tensor(372.0245, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 72 Loss: tensor(1012.5514, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 73 Loss: tensor(760.5214, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 74 Loss: tensor(13141.9189, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 75 Loss: tensor(1115.7991, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 76 Loss: tensor(8164.7798, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 77 Loss: tensor(365.1530, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 78 Loss: tensor(1224.5305, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 79 Loss: tensor(232.9897, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 80 Loss: tensor(372.7971, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 81 Loss: tensor(914.2296, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 82 Loss: tensor(638.1049, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 83 Loss: tensor(1188.2922, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 84 Loss: tensor(1499.1018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 85 Loss: tensor(1195.6184, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 86 Loss: tensor(229.9205, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 87 Loss: tensor(1321.9080, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 88 Loss: tensor(202.2255, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 89 Loss: tensor(1057.1279, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 90 Loss: tensor(743.9631, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 91 Loss: tensor(1474.5081, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 92 Loss: tensor(12983.1562, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 93 Loss: tensor(502.0119, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 94 Loss: tensor(1277.0552, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 95 Loss: tensor(1033.6121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 96 Loss: tensor(493.6984, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 97 Loss: tensor(4503.3296, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 98 Loss: tensor(510.3491, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 99 Loss: tensor(675.0135, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 100 Loss: tensor(3055.6536, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 101 Loss: tensor(2308.5679, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 102 Loss: tensor(969.1412, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 103 Loss: tensor(733.8940, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 104 Loss: tensor(375.7563, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 105 Loss: tensor(534.2463, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 106 Loss: tensor(2033.2529, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 107 Loss: tensor(1854.2566, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 108 Loss: tensor(731.6084, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 109 Loss: tensor(8523.8848, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 110 Loss: tensor(7496.6216, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 111 Loss: tensor(784.1375, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 112 Loss: tensor(932.5956, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 113 Loss: tensor(958.4615, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 114 Loss: tensor(17088.6523, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 115 Loss: tensor(3183.7002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 116 Loss: tensor(422.1781, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 117 Loss: tensor(1830.0378, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 118 Loss: tensor(1988.6360, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 119 Loss: tensor(911.1777, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 120 Loss: tensor(1512.9865, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 121 Loss: tensor(1729.1543, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 122 Loss: tensor(383.0756, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 123 Loss: tensor(370.1290, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 124 Loss: tensor(7316.7686, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 125 Loss: tensor(1199.2887, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 126 Loss: tensor(2079.7683, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 127 Loss: tensor(560.6444, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 128 Loss: tensor(297.6472, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 129 Loss: tensor(6959.8301, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 130 Loss: tensor(24041.2383, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 131 Loss: tensor(22253.7148, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 132 Loss: tensor(613.2078, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 133 Loss: tensor(426.6945, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 134 Loss: tensor(599.3439, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 135 Loss: tensor(325.7048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 136 Loss: tensor(622.3331, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 137 Loss: tensor(121.5898, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 138 Loss: tensor(458.9190, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 139 Loss: tensor(3273.7241, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 140 Loss: tensor(236.8755, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 141 Loss: tensor(185.7575, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 142 Loss: tensor(182.6647, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 143 Loss: tensor(1100.4800, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 144 Loss: tensor(7356.3633, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 145 Loss: tensor(379.7097, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 146 Loss: tensor(8395.9316, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 147 Loss: tensor(3492.7935, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 148 Loss: tensor(2565.4331, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 149 Loss: tensor(219.8169, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 150 Loss: tensor(313.5902, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 151 Loss: tensor(499.7316, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 152 Loss: tensor(1927.6924, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 153 Loss: tensor(1639.8247, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 154 Loss: tensor(334.1152, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 155 Loss: tensor(509.7204, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 156 Loss: tensor(2493.1489, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 157 Loss: tensor(636.9758, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 158 Loss: tensor(630.3615, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 159 Loss: tensor(1734.8389, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 160 Loss: tensor(890.6141, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 161 Loss: tensor(556.5378, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 162 Loss: tensor(1621.3314, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 163 Loss: tensor(971.8411, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 164 Loss: tensor(14694.9951, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 165 Loss: tensor(754.7672, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 166 Loss: tensor(13717.6104, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 167 Loss: tensor(572.2601, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 168 Loss: tensor(3646.8867, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 169 Loss: tensor(1229.7294, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 170 Loss: tensor(621.1528, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 171 Loss: tensor(629.4250, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 172 Loss: tensor(375.8890, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 173 Loss: tensor(302.4615, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 174 Loss: tensor(641.2151, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 175 Loss: tensor(284.2835, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 176 Loss: tensor(277.1270, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 177 Loss: tensor(1117.7310, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 178 Loss: tensor(427.2327, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 179 Loss: tensor(746.1941, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 180 Loss: tensor(7969.8926, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 181 Loss: tensor(105.3437, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 182 Loss: tensor(275.5066, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 183 Loss: tensor(165.2247, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 184 Loss: tensor(810.3689, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 185 Loss: tensor(233.5173, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 186 Loss: tensor(2960.8596, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 187 Loss: tensor(323.1088, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 188 Loss: tensor(128.5322, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 189 Loss: tensor(8191.9458, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 190 Loss: tensor(1119.2629, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 191 Loss: tensor(642.2699, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 192 Loss: tensor(4046.6260, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 193 Loss: tensor(2948.3813, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 194 Loss: tensor(458.7252, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 195 Loss: tensor(410.0955, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 196 Loss: tensor(707.2330, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 197 Loss: tensor(10220.5967, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 198 Loss: tensor(545.6578, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 199 Loss: tensor(267.8789, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 200 Loss: tensor(907.0050, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 201 Loss: tensor(141.8624, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 202 Loss: tensor(6192.0073, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 203 Loss: tensor(224.7978, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 204 Loss: tensor(1095.3367, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 205 Loss: tensor(706.9405, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 206 Loss: tensor(560.8202, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 207 Loss: tensor(528.2039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 208 Loss: tensor(397.9532, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 209 Loss: tensor(13202.3975, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 210 Loss: tensor(2520.0886, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 211 Loss: tensor(645.9966, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 212 Loss: tensor(2441.7764, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 213 Loss: tensor(4791.6660, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 214 Loss: tensor(1919.3938, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 215 Loss: tensor(452.5341, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 216 Loss: tensor(877.9636, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 217 Loss: tensor(442.2039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 218 Loss: tensor(1141.9738, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 219 Loss: tensor(219.5895, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 220 Loss: tensor(235.5749, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 221 Loss: tensor(944.1703, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 222 Loss: tensor(105.4302, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 223 Loss: tensor(657.2040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 224 Loss: tensor(572.7878, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 225 Loss: tensor(1137.9050, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 226 Loss: tensor(907.4506, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 227 Loss: tensor(1744.6100, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 228 Loss: tensor(225.7905, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 229 Loss: tensor(2959.3037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 230 Loss: tensor(2051.2637, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 231 Loss: tensor(427.4917, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 232 Loss: tensor(3646.8430, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 233 Loss: tensor(272.8016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 234 Loss: tensor(249.3222, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 235 Loss: tensor(204.1812, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 236 Loss: tensor(448.6537, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 237 Loss: tensor(1272.3643, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 238 Loss: tensor(410.0528, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 239 Loss: tensor(310.3413, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 240 Loss: tensor(348.4525, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 241 Loss: tensor(1669.2698, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 242 Loss: tensor(139.6846, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 243 Loss: tensor(6715.3296, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 244 Loss: tensor(2285.2524, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 245 Loss: tensor(778.2496, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 246 Loss: tensor(469.1776, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 247 Loss: tensor(428.0182, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 248 Loss: tensor(170.9752, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 249 Loss: tensor(1554.0247, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 250 Loss: tensor(601.2561, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 251 Loss: tensor(374.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 252 Loss: tensor(553.7783, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 253 Loss: tensor(7948.2949, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 254 Loss: tensor(445.7117, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 255 Loss: tensor(2337.3501, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 256 Loss: tensor(9365.8574, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 257 Loss: tensor(80.5095, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 258 Loss: tensor(1093.0155, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 259 Loss: tensor(2408.7759, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 260 Loss: tensor(371.5651, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 261 Loss: tensor(893.6829, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 262 Loss: tensor(1091.7971, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 263 Loss: tensor(383.6894, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 264 Loss: tensor(1698.2129, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 265 Loss: tensor(392.7917, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 266 Loss: tensor(688.3196, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 267 Loss: tensor(968.6207, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 268 Loss: tensor(1081.6904, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 269 Loss: tensor(324.7668, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 270 Loss: tensor(415.2481, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 271 Loss: tensor(412.5300, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 272 Loss: tensor(172.9600, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 273 Loss: tensor(403.4863, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 274 Loss: tensor(1053.9824, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 275 Loss: tensor(1627.8182, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 276 Loss: tensor(422.0964, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 277 Loss: tensor(241.4531, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 278 Loss: tensor(1080.2942, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 279 Loss: tensor(201.0201, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 280 Loss: tensor(283.5524, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 281 Loss: tensor(5635.4824, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 282 Loss: tensor(567.9468, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 283 Loss: tensor(407.9562, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 284 Loss: tensor(570.0449, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 285 Loss: tensor(281.3527, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 286 Loss: tensor(566.0978, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 287 Loss: tensor(2557.4395, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 288 Loss: tensor(421.6412, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 289 Loss: tensor(139.1989, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 290 Loss: tensor(1626.9280, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 291 Loss: tensor(4888.9424, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 292 Loss: tensor(231.5106, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 293 Loss: tensor(563.0811, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 294 Loss: tensor(473.1742, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 295 Loss: tensor(3506.3438, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 296 Loss: tensor(669.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 297 Loss: tensor(5530.5791, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 298 Loss: tensor(783.5519, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 299 Loss: tensor(240.9208, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 300 Loss: tensor(915.6223, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 301 Loss: tensor(1028.7123, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 302 Loss: tensor(439.4632, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 303 Loss: tensor(731.2589, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 304 Loss: tensor(304.3486, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 305 Loss: tensor(324.9756, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 306 Loss: tensor(1213.7020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 307 Loss: tensor(264.7334, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 308 Loss: tensor(363.4157, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 309 Loss: tensor(1151.1948, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 310 Loss: tensor(594.8949, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 311 Loss: tensor(2397.2458, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 312 Loss: tensor(4279.5186, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 313 Loss: tensor(419.5622, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 314 Loss: tensor(3498.0308, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 315 Loss: tensor(1801.3528, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 316 Loss: tensor(211.7355, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 317 Loss: tensor(1061.9565, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 318 Loss: tensor(6666.9697, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 319 Loss: tensor(4827.1323, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 320 Loss: tensor(275.3423, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 321 Loss: tensor(520.1509, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 322 Loss: tensor(506.7499, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 323 Loss: tensor(689.4743, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 324 Loss: tensor(233.7000, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 325 Loss: tensor(1333.7549, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 326 Loss: tensor(531.4034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 327 Loss: tensor(5521.1304, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 328 Loss: tensor(247.4634, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 329 Loss: tensor(262.1382, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 330 Loss: tensor(888.1266, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 331 Loss: tensor(519.4518, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 332 Loss: tensor(221.7423, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 333 Loss: tensor(684.2202, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 334 Loss: tensor(822.1138, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 335 Loss: tensor(444.1389, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 336 Loss: tensor(781.4256, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 337 Loss: tensor(1631.9492, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 338 Loss: tensor(782.2676, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 339 Loss: tensor(443.2804, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 340 Loss: tensor(16774.0410, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 341 Loss: tensor(275.2130, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 342 Loss: tensor(1629.6396, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 343 Loss: tensor(566.6102, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 344 Loss: tensor(2373.6768, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 345 Loss: tensor(4812.7520, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 346 Loss: tensor(13156.8242, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 347 Loss: tensor(4583.6309, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 348 Loss: tensor(895.0538, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 349 Loss: tensor(256.2237, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 350 Loss: tensor(39068.3594, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 351 Loss: tensor(267.0850, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 352 Loss: tensor(1073.9875, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 353 Loss: tensor(830.6603, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 354 Loss: tensor(13265.5918, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 355 Loss: tensor(225.1148, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 356 Loss: tensor(234.9284, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 357 Loss: tensor(225.4416, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 358 Loss: tensor(515.2183, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 359 Loss: tensor(337.3763, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 360 Loss: tensor(182.1408, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 361 Loss: tensor(259.5956, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 362 Loss: tensor(38630.6562, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 363 Loss: tensor(847.4777, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 364 Loss: tensor(422.0735, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 365 Loss: tensor(1101.8511, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 366 Loss: tensor(589.4921, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 367 Loss: tensor(346.5799, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 368 Loss: tensor(1639.4816, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 369 Loss: tensor(257.6069, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 370 Loss: tensor(3678.1411, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 371 Loss: tensor(228.3625, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 372 Loss: tensor(13173.5186, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 373 Loss: tensor(218.0138, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 374 Loss: tensor(191.6992, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 375 Loss: tensor(546.2125, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 376 Loss: tensor(360.2517, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 377 Loss: tensor(260.5531, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 378 Loss: tensor(3653.2505, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 379 Loss: tensor(3183.4155, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 380 Loss: tensor(481.8200, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 381 Loss: tensor(2373.2161, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 382 Loss: tensor(4563.8940, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 383 Loss: tensor(1891.0114, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 384 Loss: tensor(308.0446, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 385 Loss: tensor(11301.2100, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 386 Loss: tensor(5241.6250, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 387 Loss: tensor(6663.6768, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 388 Loss: tensor(427.1788, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 389 Loss: tensor(460.7085, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 390 Loss: tensor(561.4120, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 391 Loss: tensor(11920.0879, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 392 Loss: tensor(374.7825, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 393 Loss: tensor(272.7946, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 394 Loss: tensor(567.6360, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 395 Loss: tensor(2110.1670, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 396 Loss: tensor(2696.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 397 Loss: tensor(1361.4327, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 398 Loss: tensor(404.6464, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 399 Loss: tensor(568.9201, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 400 Loss: tensor(568.5122, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 401 Loss: tensor(629.3950, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 402 Loss: tensor(427.9677, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 403 Loss: tensor(341.4096, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 404 Loss: tensor(551.7026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 405 Loss: tensor(203.8049, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 406 Loss: tensor(785.6857, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 407 Loss: tensor(906.7568, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 408 Loss: tensor(580.3861, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 409 Loss: tensor(111.8826, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 410 Loss: tensor(400.8969, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 411 Loss: tensor(6830.4907, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 412 Loss: tensor(786.4192, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 413 Loss: tensor(4342.5161, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 414 Loss: tensor(260.8894, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 415 Loss: tensor(665.7414, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 416 Loss: tensor(203.0611, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 417 Loss: tensor(4084.9011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 418 Loss: tensor(13611.9844, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 419 Loss: tensor(735.3571, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 420 Loss: tensor(1691.5171, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 421 Loss: tensor(5541.6035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 422 Loss: tensor(553.8149, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 423 Loss: tensor(449.5292, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 424 Loss: tensor(290.1503, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 425 Loss: tensor(727.8392, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 426 Loss: tensor(183.5388, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 427 Loss: tensor(1072.2507, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 428 Loss: tensor(900.3019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 429 Loss: tensor(477.8156, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 430 Loss: tensor(3648.2268, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 431 Loss: tensor(230.0226, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 432 Loss: tensor(1269.9069, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 433 Loss: tensor(604.1003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 434 Loss: tensor(4234.2207, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 435 Loss: tensor(234.2557, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 436 Loss: tensor(309.1748, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 437 Loss: tensor(547.3842, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 438 Loss: tensor(2764.6121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 439 Loss: tensor(1381.8434, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 440 Loss: tensor(203.8287, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 441 Loss: tensor(4084.4089, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 442 Loss: tensor(1005.7360, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 443 Loss: tensor(663.7505, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 444 Loss: tensor(11973.2754, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 445 Loss: tensor(441.8423, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 446 Loss: tensor(1654.0500, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 447 Loss: tensor(389.7236, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 448 Loss: tensor(7417.6387, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 449 Loss: tensor(1743.7913, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 450 Loss: tensor(1402.7175, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 451 Loss: tensor(799.6495, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 452 Loss: tensor(666.1039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 453 Loss: tensor(261.0297, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 454 Loss: tensor(175.6603, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 455 Loss: tensor(223.4018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 456 Loss: tensor(318.6387, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 457 Loss: tensor(203.1420, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 458 Loss: tensor(16696.1543, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 459 Loss: tensor(641.3132, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 460 Loss: tensor(336.3347, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 461 Loss: tensor(888.5968, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 462 Loss: tensor(11970.7598, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 463 Loss: tensor(494.6207, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 464 Loss: tensor(385.9273, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 465 Loss: tensor(526.4138, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 466 Loss: tensor(2695.7734, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 467 Loss: tensor(934.8284, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 468 Loss: tensor(2698.5415, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 469 Loss: tensor(591.3821, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 470 Loss: tensor(5448.7949, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 471 Loss: tensor(274.0458, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 472 Loss: tensor(516.2863, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 473 Loss: tensor(1189.0834, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 474 Loss: tensor(1743.8900, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 475 Loss: tensor(3689.3462, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 476 Loss: tensor(3660.9565, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 477 Loss: tensor(841.6244, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 478 Loss: tensor(3478.0654, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 479 Loss: tensor(180.7168, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 480 Loss: tensor(1132.9999, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 481 Loss: tensor(553.5953, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 482 Loss: tensor(657.0186, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 483 Loss: tensor(2109.9255, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 484 Loss: tensor(1691.4180, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 485 Loss: tensor(528.7470, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 486 Loss: tensor(1072.3108, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 487 Loss: tensor(1070.1517, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 488 Loss: tensor(917.0416, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 489 Loss: tensor(456.2060, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 490 Loss: tensor(211.8399, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 491 Loss: tensor(1036.6338, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 492 Loss: tensor(535.0591, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 493 Loss: tensor(1654.0081, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 494 Loss: tensor(1076.7803, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 495 Loss: tensor(5303.0586, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 496 Loss: tensor(520.7690, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 497 Loss: tensor(2530.5850, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 498 Loss: tensor(619.0173, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 499 Loss: tensor(1126.8556, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 500 Loss: tensor(248.1649, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 501 Loss: tensor(560.8015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 502 Loss: tensor(690.2656, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 503 Loss: tensor(463.0738, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 504 Loss: tensor(502.0929, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 505 Loss: tensor(248.1244, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 506 Loss: tensor(172.6299, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 507 Loss: tensor(1080.9307, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 508 Loss: tensor(452.4162, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 509 Loss: tensor(5469.5869, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 510 Loss: tensor(418.0689, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 511 Loss: tensor(3672.1125, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 512 Loss: tensor(851.4048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 513 Loss: tensor(1494.2626, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 514 Loss: tensor(1188.8405, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 515 Loss: tensor(228.8276, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 516 Loss: tensor(524.1563, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 517 Loss: tensor(421.1639, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 518 Loss: tensor(1087.9011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 519 Loss: tensor(364.5368, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 520 Loss: tensor(853.6761, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 521 Loss: tensor(250.4005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 522 Loss: tensor(866.7720, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 523 Loss: tensor(895.2690, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 524 Loss: tensor(606.6576, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 525 Loss: tensor(1048.9385, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 526 Loss: tensor(1048.9131, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 527 Loss: tensor(697.9452, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 528 Loss: tensor(223.4977, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 529 Loss: tensor(518.7546, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 530 Loss: tensor(960.0517, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 531 Loss: tensor(616.3430, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 532 Loss: tensor(3647.2627, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 533 Loss: tensor(1114.0911, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 534 Loss: tensor(1048.8419, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 535 Loss: tensor(5469.8950, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 536 Loss: tensor(4203.4946, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 537 Loss: tensor(663.8010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 538 Loss: tensor(2010.1941, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 539 Loss: tensor(261.0517, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 540 Loss: tensor(2856.2485, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 541 Loss: tensor(786.9167, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 542 Loss: tensor(845.1249, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 543 Loss: tensor(424.9572, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 544 Loss: tensor(1227.0345, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 545 Loss: tensor(1890.6372, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 546 Loss: tensor(502.4282, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 547 Loss: tensor(616.3236, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 548 Loss: tensor(340.4383, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 549 Loss: tensor(2394.2236, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 550 Loss: tensor(931.3824, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 551 Loss: tensor(616.4470, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 552 Loss: tensor(3595.0525, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 553 Loss: tensor(5237.4204, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 554 Loss: tensor(615.2163, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 555 Loss: tensor(250.3560, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 556 Loss: tensor(348.3006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 557 Loss: tensor(168.2165, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 558 Loss: tensor(367.0854, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 559 Loss: tensor(806.1192, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 560 Loss: tensor(276.1630, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 561 Loss: tensor(857.8392, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 562 Loss: tensor(439.5344, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 563 Loss: tensor(1888.8683, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 564 Loss: tensor(5326.4355, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 565 Loss: tensor(435.3686, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 566 Loss: tensor(641.7872, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 567 Loss: tensor(449.1014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 568 Loss: tensor(709.4497, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 569 Loss: tensor(2391.6938, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 570 Loss: tensor(3402.3044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 571 Loss: tensor(489.5640, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 572 Loss: tensor(1103.9911, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 573 Loss: tensor(1816.0765, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 574 Loss: tensor(4083.9148, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 575 Loss: tensor(489.5533, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 576 Loss: tensor(2028.4779, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 577 Loss: tensor(1079.7390, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 578 Loss: tensor(1188.9603, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 579 Loss: tensor(727.7042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 580 Loss: tensor(2109.6855, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 581 Loss: tensor(535.8270, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 582 Loss: tensor(569.0577, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 583 Loss: tensor(786.7576, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 584 Loss: tensor(378.0951, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 585 Loss: tensor(757.9203, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 586 Loss: tensor(172.6338, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 587 Loss: tensor(2010.1375, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 588 Loss: tensor(1353.2697, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 589 Loss: tensor(520.6638, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 590 Loss: tensor(103.4968, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 591 Loss: tensor(648.4217, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 592 Loss: tensor(435.4146, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 593 Loss: tensor(251.0251, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 594 Loss: tensor(568.6398, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 595 Loss: tensor(238.8859, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 596 Loss: tensor(321.1265, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 597 Loss: tensor(3592.1812, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 598 Loss: tensor(482.7867, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 599 Loss: tensor(806.2059, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 600 Loss: tensor(463.6753, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 601 Loss: tensor(339.9563, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 602 Loss: tensor(609.0422, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 603 Loss: tensor(1288.8911, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 604 Loss: tensor(4051.6895, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 605 Loss: tensor(1302.4362, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 606 Loss: tensor(2856.3250, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 607 Loss: tensor(375.2271, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 608 Loss: tensor(903.8455, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 609 Loss: tensor(435.4546, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 610 Loss: tensor(459.4636, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 611 Loss: tensor(2919.8589, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 612 Loss: tensor(7415.7432, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 613 Loss: tensor(959.9606, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 614 Loss: tensor(248.0940, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 615 Loss: tensor(494.4337, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 616 Loss: tensor(4340.6567, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 617 Loss: tensor(2083.5132, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 618 Loss: tensor(516.0914, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 619 Loss: tensor(486.0287, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 620 Loss: tensor(260.5608, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 621 Loss: tensor(9977.9121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 622 Loss: tensor(336.3135, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 623 Loss: tensor(2024.0836, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 624 Loss: tensor(316.8709, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 625 Loss: tensor(416.0744, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 626 Loss: tensor(299.6843, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 627 Loss: tensor(111.9607, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 628 Loss: tensor(2028.7130, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 629 Loss: tensor(398.5193, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 630 Loss: tensor(524.0078, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 631 Loss: tensor(1254.3207, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 632 Loss: tensor(1494.1588, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 633 Loss: tensor(436.1520, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 634 Loss: tensor(727.7120, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 635 Loss: tensor(459.4517, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 636 Loss: tensor(1302.5657, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 637 Loss: tensor(946.9456, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 638 Loss: tensor(380.3771, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 639 Loss: tensor(621.7773, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 640 Loss: tensor(584.4208, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 641 Loss: tensor(175.6943, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 642 Loss: tensor(341.5446, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 643 Loss: tensor(2028.5120, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 644 Loss: tensor(651.6226, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 645 Loss: tensor(485.4138, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 646 Loss: tensor(791.9836, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 647 Loss: tensor(1552.3538, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 648 Loss: tensor(663.8112, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 649 Loss: tensor(4340.6709, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 650 Loss: tensor(697.8699, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 651 Loss: tensor(230.0341, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 652 Loss: tensor(7909.3174, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 653 Loss: tensor(648.4120, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 654 Loss: tensor(185.8806, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 655 Loss: tensor(1526.4823, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 656 Loss: tensor(904.5212, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 657 Loss: tensor(248.1048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 658 Loss: tensor(680.7744, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 659 Loss: tensor(8993.3184, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 660 Loss: tensor(2932.3416, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 661 Loss: tensor(1491.5876, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 662 Loss: tensor(1381.8008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 663 Loss: tensor(651.6193, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 664 Loss: tensor(531.9150, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 665 Loss: tensor(172.6124, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 666 Loss: tensor(197.4007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 667 Loss: tensor(5553.3086, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 668 Loss: tensor(2180.9263, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 669 Loss: tensor(864.7416, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 670 Loss: tensor(210.7346, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 671 Loss: tensor(361.2088, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 672 Loss: tensor(515.6587, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 673 Loss: tensor(1909.1411, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 674 Loss: tensor(2010.3688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 675 Loss: tensor(788.6155, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 676 Loss: tensor(207.3493, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 677 Loss: tensor(754.7712, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 678 Loss: tensor(735.2355, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 679 Loss: tensor(58.8972, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 680 Loss: tensor(3862.1511, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 681 Loss: tensor(962.4020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 682 Loss: tensor(786.8209, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 683 Loss: tensor(261.0135, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 684 Loss: tensor(175.9142, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 685 Loss: tensor(1226.8499, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 686 Loss: tensor(873.5656, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 687 Loss: tensor(2919.9290, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 688 Loss: tensor(4083.9150, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 689 Loss: tensor(1005.8537, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 690 Loss: tensor(103.5340, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 691 Loss: tensor(257.8043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 692 Loss: tensor(417.9664, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 693 Loss: tensor(542.4338, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 694 Loss: tensor(5853.1465, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 695 Loss: tensor(389.9019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 696 Loss: tensor(1087.3835, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 697 Loss: tensor(487.0877, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 698 Loss: tensor(1917.2942, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 699 Loss: tensor(2040.0977, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 700 Loss: tensor(417.6437, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 701 Loss: tensor(1917.3042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 702 Loss: tensor(1022.7019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 703 Loss: tensor(580.8655, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 704 Loss: tensor(425.1063, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 705 Loss: tensor(919.4957, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 706 Loss: tensor(616.3518, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 707 Loss: tensor(1769.3682, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 708 Loss: tensor(309.1706, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 709 Loss: tensor(1302.5876, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 710 Loss: tensor(1361.4447, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 711 Loss: tensor(971.4113, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 712 Loss: tensor(643.0164, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 713 Loss: tensor(414.9110, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 714 Loss: tensor(7575.4551, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 715 Loss: tensor(6825.2393, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 716 Loss: tensor(289.0083, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 717 Loss: tensor(26538.0371, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 718 Loss: tensor(7029.4590, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 719 Loss: tensor(2343.2280, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 720 Loss: tensor(2024.0986, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 721 Loss: tensor(140.6631, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 722 Loss: tensor(666.1028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 723 Loss: tensor(3501.3152, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 724 Loss: tensor(19407.6523, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 725 Loss: tensor(931.3463, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 726 Loss: tensor(1269.7554, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 727 Loss: tensor(3595.2559, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 728 Loss: tensor(3540.7026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 729 Loss: tensor(3317.0391, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 730 Loss: tensor(1640.4586, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 731 Loss: tensor(578.6378, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 732 Loss: tensor(606.6141, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 733 Loss: tensor(486.1136, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 734 Loss: tensor(4522.2539, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 735 Loss: tensor(3689.3938, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 736 Loss: tensor(273.6915, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 737 Loss: tensor(1743.7039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 738 Loss: tensor(498.6862, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 739 Loss: tensor(674.4050, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 740 Loss: tensor(785.9891, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 741 Loss: tensor(3776.8955, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 742 Loss: tensor(13345.3721, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 743 Loss: tensor(229.6759, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 744 Loss: tensor(7575.8047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 745 Loss: tensor(1508.0481, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 746 Loss: tensor(277.1415, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 747 Loss: tensor(1136.7179, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 748 Loss: tensor(1233.1351, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 749 Loss: tensor(888.5411, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 750 Loss: tensor(290.0883, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 751 Loss: tensor(2028.8783, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 752 Loss: tensor(656.9038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 753 Loss: tensor(320.5388, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 754 Loss: tensor(542.0432, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 755 Loss: tensor(3179.1265, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 756 Loss: tensor(494.6608, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 757 Loss: tensor(16692.2539, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 758 Loss: tensor(604.1426, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 759 Loss: tensor(125.7402, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 760 Loss: tensor(389.9488, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 761 Loss: tensor(1494.1675, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 762 Loss: tensor(181.1322, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 763 Loss: tensor(4051.4746, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 764 Loss: tensor(290.0561, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 765 Loss: tensor(3776.9932, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 766 Loss: tensor(866.8455, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 767 Loss: tensor(853.5068, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 768 Loss: tensor(6992.2725, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 769 Loss: tensor(580.9354, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 770 Loss: tensor(2024.0181, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 771 Loss: tensor(845.1505, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 772 Loss: tensor(278.2000, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 773 Loss: tensor(489.6332, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 774 Loss: tensor(641.2952, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 775 Loss: tensor(446.3752, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 776 Loss: tensor(946.8666, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 777 Loss: tensor(1640.6166, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 778 Loss: tensor(674.3570, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 779 Loss: tensor(787.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 780 Loss: tensor(2484.3130, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 781 Loss: tensor(732.3093, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 782 Loss: tensor(535.7289, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 783 Loss: tensor(207.3611, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 784 Loss: tensor(3146.9241, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 785 Loss: tensor(9350.2568, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 786 Loss: tensor(1126.8286, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 787 Loss: tensor(881.6792, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 788 Loss: tensor(455.7885, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 789 Loss: tensor(1255.6270, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 790 Loss: tensor(172.6526, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 791 Loss: tensor(606.1445, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 792 Loss: tensor(895.2209, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 793 Loss: tensor(621.8323, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 794 Loss: tensor(364.3108, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 795 Loss: tensor(526.3365, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 796 Loss: tensor(7696.4570, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 797 Loss: tensor(895.2585, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 798 Loss: tensor(824.5778, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 799 Loss: tensor(1666.8773, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 800 Loss: tensor(4522.5410, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 801 Loss: tensor(1917.4318, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 802 Loss: tensor(741.8168, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 803 Loss: tensor(228.1714, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 804 Loss: tensor(175.6528, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 805 Loss: tensor(7909.2793, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 806 Loss: tensor(294.7284, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 807 Loss: tensor(3478.1489, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 808 Loss: tensor(11907.0078, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 809 Loss: tensor(223.5329, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 810 Loss: tensor(228.1927, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 811 Loss: tensor(276.1865, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 812 Loss: tensor(1793.4260, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 813 Loss: tensor(652.4760, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 814 Loss: tensor(414.8644, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 815 Loss: tensor(847.2790, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 816 Loss: tensor(1502.6062, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 817 Loss: tensor(140.6854, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 818 Loss: tensor(263.0919, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 819 Loss: tensor(1608.2620, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 820 Loss: tensor(516.0160, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 821 Loss: tensor(344.4525, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 822 Loss: tensor(2140.9136, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 823 Loss: tensor(1264.8140, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 824 Loss: tensor(238.9175, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 825 Loss: tensor(936.5466, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 826 Loss: tensor(674.3988, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 827 Loss: tensor(391.6342, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 828 Loss: tensor(895.2041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 829 Loss: tensor(1102.7950, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 830 Loss: tensor(346.2432, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 831 Loss: tensor(2028.9600, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 832 Loss: tensor(364.4545, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 833 Loss: tensor(406.1581, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 834 Loss: tensor(341.4510, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 835 Loss: tensor(526.3625, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 836 Loss: tensor(3647.6318, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 837 Loss: tensor(856.7479, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 838 Loss: tensor(889.9484, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 839 Loss: tensor(542.0339, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 840 Loss: tensor(425.1020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 841 Loss: tensor(172.6668, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 842 Loss: tensor(565.7920, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 843 Loss: tensor(740.0121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 844 Loss: tensor(250.4162, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 845 Loss: tensor(5677.1538, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 846 Loss: tensor(757.8979, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 847 Loss: tensor(340.4297, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 848 Loss: tensor(1072.1605, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 849 Loss: tensor(356.1519, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 850 Loss: tensor(455.8300, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 851 Loss: tensor(5448.8560, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 852 Loss: tensor(3314.1670, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 853 Loss: tensor(666.0832, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 854 Loss: tensor(248.1409, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 855 Loss: tensor(528.6879, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 856 Loss: tensor(12775.6758, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 857 Loss: tensor(1320.8619, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 858 Loss: tensor(404.7161, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 859 Loss: tensor(518.7378, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 860 Loss: tensor(269.6614, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 861 Loss: tensor(1381.8948, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 862 Loss: tensor(797.3086, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 863 Loss: tensor(1381.9033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 864 Loss: tensor(80.4991, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 865 Loss: tensor(927.7368, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 866 Loss: tensor(380.8316, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 867 Loss: tensor(166.5457, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 868 Loss: tensor(889.8029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 869 Loss: tensor(219.7687, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 870 Loss: tensor(277.2067, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 871 Loss: tensor(3776.6650, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 872 Loss: tensor(26538.7031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 873 Loss: tensor(352.8580, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 874 Loss: tensor(1008.4365, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 875 Loss: tensor(271.6029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 876 Loss: tensor(770.3898, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 877 Loss: tensor(634.1321, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 878 Loss: tensor(183.5710, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 879 Loss: tensor(465.3166, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 880 Loss: tensor(261.3369, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 881 Loss: tensor(452.3709, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 882 Loss: tensor(425.3069, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 883 Loss: tensor(378.1614, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 884 Loss: tensor(560.7452, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 885 Loss: tensor(424.4988, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 886 Loss: tensor(203.8313, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 887 Loss: tensor(628.0850, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 888 Loss: tensor(3862.2942, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 889 Loss: tensor(436.8964, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 890 Loss: tensor(2856.1904, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 891 Loss: tensor(494.4781, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 892 Loss: tensor(272.7576, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 893 Loss: tensor(406.2485, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 894 Loss: tensor(251.0454, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 895 Loss: tensor(1654.0283, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 896 Loss: tensor(498.6606, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 897 Loss: tensor(971.3923, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 898 Loss: tensor(348.3932, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 899 Loss: tensor(873.5955, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 900 Loss: tensor(697.8721, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 901 Loss: tensor(1453.1423, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 902 Loss: tensor(58.9082, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 903 Loss: tensor(553.4286, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 904 Loss: tensor(936.7959, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 905 Loss: tensor(900.1570, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 906 Loss: tensor(321.0942, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 907 Loss: tensor(534.8723, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 908 Loss: tensor(271.6353, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 909 Loss: tensor(385.9074, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 910 Loss: tensor(380.8422, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 911 Loss: tensor(5142.6963, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 912 Loss: tensor(502.0479, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 913 Loss: tensor(1299.6143, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 914 Loss: tensor(252.1145, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 915 Loss: tensor(455.8013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 916 Loss: tensor(478.4822, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 917 Loss: tensor(435.4816, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 918 Loss: tensor(211.8295, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 919 Loss: tensor(397.8414, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 920 Loss: tensor(697.8569, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 921 Loss: tensor(656.9050, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 922 Loss: tensor(336.3036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 923 Loss: tensor(494.4241, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 924 Loss: tensor(321.1223, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 925 Loss: tensor(168.1899, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 926 Loss: tensor(4522.4053, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 927 Loss: tensor(701.8718, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 928 Loss: tensor(318.6808, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 929 Loss: tensor(1076.7698, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 930 Loss: tensor(1102.7147, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 931 Loss: tensor(938.9744, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 932 Loss: tensor(248.1135, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 933 Loss: tensor(250.4186, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 934 Loss: tensor(257.6093, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 935 Loss: tensor(1402.6217, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 936 Loss: tensor(352.8679, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 937 Loss: tensor(309.1732, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 938 Loss: tensor(320.6054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 939 Loss: tensor(477.8409, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 940 Loss: tensor(628.1569, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 941 Loss: tensor(2329.5388, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 942 Loss: tensor(531.9901, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 943 Loss: tensor(218.5917, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 944 Loss: tensor(404.7471, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 945 Loss: tensor(1793.6826, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 946 Loss: tensor(616.4142, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 947 Loss: tensor(4741.8765, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 948 Loss: tensor(1087.4739, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 949 Loss: tensor(907.2724, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 950 Loss: tensor(786.7710, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 951 Loss: tensor(1063.8477, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 952 Loss: tensor(799.5853, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 953 Loss: tensor(1494.1724, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 954 Loss: tensor(258.9775, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 955 Loss: tensor(1608.0798, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 956 Loss: tensor(1126.7441, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 957 Loss: tensor(5676.9561, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 958 Loss: tensor(214.0620, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 959 Loss: tensor(1640.1085, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 960 Loss: tensor(1526.3547, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 961 Loss: tensor(514.1878, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 962 Loss: tensor(1193.2983, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 963 Loss: tensor(622.9214, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 964 Loss: tensor(473.0803, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 965 Loss: tensor(5540.9326, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m low_res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(low_res)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m posterior_initial \u001b[38;5;241m=\u001b[39m sample_p_0(low_res)\n\u001b[0;32m---> 31\u001b[0m posterior_final \u001b[38;5;241m=\u001b[39m \u001b[43mula_posterior_preconditioner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposterior_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_high\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optG\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m out \u001b[38;5;241m=\u001b[39m G(posterior_final\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,N_high,N_high))\u001b[38;5;241m.\u001b[39mreshape(N_low,N_low)\n",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m, in \u001b[0;36mula_posterior_preconditioner\u001b[0;34m(z, b_high, x, G)\u001b[0m\n\u001b[1;32m     29\u001b[0m x_hat \u001b[38;5;241m=\u001b[39m G(z\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,N_high,N_high))\u001b[38;5;241m.\u001b[39mreshape(N_low,N_low)\n\u001b[1;32m     30\u001b[0m log_likelihood \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mmath\u001b[38;5;241m.\u001b[39mpow(ll_sigma, \u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul((x\u001b[38;5;241m-\u001b[39mx_hat)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,N_low\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m),(x\u001b[38;5;241m-\u001b[39mx_hat)\u001b[38;5;241m.\u001b[39mreshape(N_low\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m---> 31\u001b[0m grad_ll \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_likelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Grad prior\u001b[39;00m\n\u001b[1;32m     34\u001b[0m difference \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mspmm(A_high,z\u001b[38;5;241m.\u001b[39mreshape(N_high\u001b[38;5;241m*\u001b[39mN_high,\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m-\u001b[39m b_high\u001b[38;5;241m.\u001b[39mreshape(N_high\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt121/lib/python3.9/site-packages/torch/autograd/__init__.py:412\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    408\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    409\u001b[0m         grad_outputs_\n\u001b[1;32m    410\u001b[0m     )\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    423\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    425\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/envs/pt121/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train with sampled data\n",
    "epoch_num = 1000\n",
    "lr = 0.003\n",
    "gamma = 0.5\n",
    "step_size = 30\n",
    "minimum_loss = float('inf')\n",
    "loss_track = []\n",
    "\n",
    "K = 70\n",
    "s = 0.0004\n",
    "\n",
    "G = DownScale()\n",
    "G.apply(weights_init_xavier).to(device)\n",
    "mse = nn.MSELoss(reduction='sum')\n",
    "optG = torch.optim.Adam(G.parameters(), lr = lr, weight_decay=0, betas=(0.5, 0.999))\n",
    "r_scheduleG = torch.optim.lr_scheduler.StepLR(optG, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Logger info\n",
    "dir_name = f'models/model1/40_160/lr{lr}_gamma{gamma}_stepsize{step_size}_K{K}_llsigma_{ll_sigma}_psigma{prior_sigma}'\n",
    "makedir(dir_name)\n",
    "logger = setup_logging('job0', dir_name, console=True)\n",
    "logger.info(f'Training for {epoch_num} epoches and learning rate is {lr}')\n",
    "\n",
    "for epoch in range(1, epoch_num+1):\n",
    "    \n",
    "    b_high, low_res = sample_data()\n",
    "    b_high = torch.tensor(b_high).to(torch.float32).to(device)\n",
    "    low_res = torch.tensor(low_res).to(torch.float32).to(device)\n",
    "    \n",
    "    posterior_initial = sample_p_0(low_res)\n",
    "    posterior_final = ula_posterior_preconditioner(posterior_initial, b_high, low_res, G)\n",
    "\n",
    "    optG.zero_grad()\n",
    "    \n",
    "    out = G(posterior_final.reshape(1,N_high,N_high)).reshape(N_low,N_low)\n",
    "    loss = mse(out,low_res)\n",
    "    \n",
    "    loss.backward()\n",
    "    optG.step()\n",
    "    \n",
    "    if loss < minimum_loss:\n",
    "        save_model(dir_name, epoch, 'best_model', r_scheduleG, G, optG)\n",
    "        minimum_loss = loss\n",
    "            \n",
    "    if epoch%100 == 0:\n",
    "        save_model(dir_name, epoch, 'model_epoch_{}'.format(epoch), r_scheduleG, G, optG)\n",
    "    \n",
    "    save_model(dir_name, epoch, 'current_epoch', r_scheduleG, G, optG)\n",
    "    loss_track.append(loss.cpu().data.numpy())\n",
    "    np.save(f'{dir_name}/chains/loss_curve.npy', np.array(loss_track))\n",
    "    \n",
    "    print(\"Epoch:\", epoch, \"Loss:\", loss)\n",
    "\n",
    "    r_scheduleG.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
