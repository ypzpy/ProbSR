{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS']='2'\n",
    "os.environ['LD_LIBRARY_PATH']=''\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pz281@ad.eng.cam.ac.uk/mnt/PhD/Pro_Down_SR\n"
     ]
    }
   ],
   "source": [
    "%cd /home/pz281@ad.eng.cam.ac.uk/mnt/PhD/Pro_Down_SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pz281@ad.eng.cam.ac.uk/anaconda3/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data_generation import *\n",
    "from scipy.linalg import sqrtm\n",
    "from downscaling import *\n",
    "from utils import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pz281@ad.eng.cam.ac.uk/mnt/PhD/Pro_Down_SR/Matrix_Downsampling\n"
     ]
    }
   ],
   "source": [
    "%cd Matrix_Downsampling/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langevin & Training Downscale Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upscale By 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_low = 21\n",
    "N_high = 81\n",
    "scale = 4\n",
    "a,b,c = 8,5,5\n",
    "\n",
    "h_low = 1/(N_low-1)\n",
    "x_low = np.arange(0,1.0001,h_low)\n",
    "y_low = np.arange(0,1.0001,h_low)\n",
    "\n",
    "h_high = 1/(N_high-1)\n",
    "x_high = np.arange(0,1.0001,h_high)\n",
    "y_high = np.arange(0,1.0001,h_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_high = create_A(N_high)\n",
    "A_low = create_A(N_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code downscaling matrix\n",
    "H = np.zeros((N_low*N_low, N_high*N_high))\n",
    "\n",
    "submatrix = np.zeros((N_low,N_high))\n",
    "for i in range(N_low):\n",
    "    submatrix[i,scale*i] = 1\n",
    "    \n",
    "for j in range(N_low):\n",
    "    H[N_low*j:N_low*(j+1),N_high*scale*j:N_high*(scale*j+1)] = submatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Parameters for RBF kernal\n",
    "prior_sigma = 0.002\n",
    "ll_sigma = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.eye(N_high**2) * prior_sigma**2\n",
    "G_inverse = np.eye(N_high**2) * (1/prior_sigma**2)\n",
    "\n",
    "# Code downscaling matrix\n",
    "H = np.zeros((N_low*N_low, N_high*N_high))\n",
    "\n",
    "submatrix = np.zeros((N_low,N_high))\n",
    "for i in range(N_low):\n",
    "    submatrix[i,scale*i] = 1\n",
    "    \n",
    "for j in range(N_low):\n",
    "    H[N_low*j:N_low*(j+1),N_high*scale*j:N_high*(scale*j+1)] = submatrix\n",
    "    \n",
    "# Turn matrices to tensors\n",
    "G = torch.tensor(G).to(torch.float32).to(device)\n",
    "G_inverse = torch.tensor(G_inverse).to(torch.float32).to(device)\n",
    "H = torch.tensor(H).to(torch.float32).to(device)\n",
    "A_high = torch.tensor(create_A(N_high)).to(torch.float32).to(device)\n",
    "b_high = torch.tensor(create_forcing_term(N_high,a,b,c)).to(torch.float32).to(device)\n",
    "\n",
    "# Store sparse matrices as sparse tensor\n",
    "A_high = A_high.to_sparse()\n",
    "H = H.to_sparse()\n",
    "G = G.to_sparse()\n",
    "G_inverse = G_inverse.to_sparse()\n",
    "operator = torch.spmm(A_high.T,G_inverse).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataFromH5File4(\"/home/pz281@ad.eng.cam.ac.uk/mnt/PhD/Pro_Down_SR/data/21_81_low_forcing.h5\")\n",
    "\n",
    "trainset = random.sample(range(0, 128), 100)\n",
    "testset = [i for i in range(0,128) if i not in trainset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data():\n",
    "    coefficient = random.sample(trainset,1)[0]\n",
    "    forcing = dataset[coefficient][0]\n",
    "    lr = dataset[coefficient][1]\n",
    "    \n",
    "    return forcing, lr\n",
    "\n",
    "\n",
    "def sample_p_0():\n",
    "    # Randomly sampling for initialisation of the Langevin dynamics\n",
    "    # prior = torch.randn(*[batch_size,1,20,20]).to(device)\n",
    "    \n",
    "    # Set the u_low_mean to the initialisation of the Langevin dynamics\n",
    "    posterior_initial = torch.randn([N_high,N_high]).to(torch.float32)\n",
    "    posterior_initial = torch.tensor(posterior_initial).to(device).to(torch.float32)\n",
    "    \n",
    "    return posterior_initial\n",
    "\n",
    "    \n",
    "def ula_posterior_preconditioner(z, b_high, x, G):\n",
    "    \"\"\"\n",
    "    Langevin dynamics with preconditioner\n",
    "    \"\"\"\n",
    "    z = z.clone().detach().requires_grad_(True)\n",
    "    for i in range(K):\n",
    "        # Grad log-likelihood\n",
    "        observation = torch.spmm(H,z.reshape(N_high*N_high,1)).reshape(N_low,N_low)\n",
    "        x_hat = observation + G(observation.reshape(1,N_low,N_low)).reshape(N_low,N_low)\n",
    "        log_likelihood = (-1/(2*math.pow(ll_sigma, 2)) * torch.matmul((x-x_hat).reshape(1,N_low**2),(x-x_hat).reshape(N_low**2,1)))\n",
    "        grad_ll = torch.autograd.grad(log_likelihood, z)[0]\n",
    "\n",
    "        # Grad prior\n",
    "        difference = torch.spmm(A_high,z.reshape(N_high*N_high,1)) - b_high.reshape(N_high**2,1)\n",
    "        # log_prior = - 0.5 * difference.T @ G_inverse @ difference\n",
    "        # grad_log_prior = torch.autograd.grad(log_prior, z)[0]\n",
    "        grad_log_prior = (- torch.spmm(operator,difference)).reshape(N_high,N_high)\n",
    "        \n",
    "        # Random noise term\n",
    "        W = torch.randn(*[N_high,N_high]).to(device)\n",
    "        # random = torch.matmul(G_sqrt,W.reshape(N_high**2,1)).reshape(N_high,N_high)\n",
    "        \n",
    "        z = z + 0.5 * s ** 2 * grad_log_prior + 0.5 * s ** 2 * grad_ll + s * W\n",
    "        # chains_evolution.append(z.cpu().data.numpy())   \n",
    "           \n",
    "    return z.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-02 23:09:17,954 : Training for 1000 epoches and learning rate is 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6282/2499179023.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  posterior_initial = torch.tensor(posterior_initial).to(device).to(torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: tensor(0.0067, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 2 Loss: tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 3 Loss: tensor(0.0070, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 4 Loss: tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 5 Loss: tensor(0.0054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 6 Loss: tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 7 Loss: tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 8 Loss: tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 9 Loss: tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 10 Loss: tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 11 Loss: tensor(0.0055, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 12 Loss: tensor(0.0097, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 13 Loss: tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 14 Loss: tensor(0.0090, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 15 Loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 16 Loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 17 Loss: tensor(0.0068, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 18 Loss: tensor(0.0065, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 19 Loss: tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 20 Loss: tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 21 Loss: tensor(0.0096, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 22 Loss: tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 23 Loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 24 Loss: tensor(0.0336, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 25 Loss: tensor(0.0050, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 26 Loss: tensor(0.0053, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 27 Loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 28 Loss: tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 29 Loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 30 Loss: tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 31 Loss: tensor(0.0069, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 32 Loss: tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 33 Loss: tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 34 Loss: tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 35 Loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 36 Loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 37 Loss: tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 38 Loss: tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 39 Loss: tensor(0.0059, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 40 Loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 41 Loss: tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 42 Loss: tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 43 Loss: tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 44 Loss: tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 45 Loss: tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 46 Loss: tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 47 Loss: tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 48 Loss: tensor(0.0093, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 49 Loss: tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 50 Loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 51 Loss: tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 52 Loss: tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 53 Loss: tensor(0.0075, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 54 Loss: tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 55 Loss: tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 56 Loss: tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 57 Loss: tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 58 Loss: tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 59 Loss: tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 60 Loss: tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 61 Loss: tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 62 Loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 63 Loss: tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 64 Loss: tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 65 Loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 66 Loss: tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 67 Loss: tensor(0.0078, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 68 Loss: tensor(0.0080, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 69 Loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 70 Loss: tensor(0.0112, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 71 Loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 72 Loss: tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 73 Loss: tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 74 Loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 75 Loss: tensor(0.0072, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 76 Loss: tensor(0.0313, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 77 Loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 78 Loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 79 Loss: tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 80 Loss: tensor(0.0312, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 81 Loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 82 Loss: tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 83 Loss: tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 84 Loss: tensor(0.0082, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 85 Loss: tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 86 Loss: tensor(0.0073, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 87 Loss: tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 88 Loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 89 Loss: tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 90 Loss: tensor(0.0111, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 91 Loss: tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 92 Loss: tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 93 Loss: tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 94 Loss: tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 95 Loss: tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 96 Loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 97 Loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 98 Loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 99 Loss: tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 100 Loss: tensor(0.0169, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 101 Loss: tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 102 Loss: tensor(0.0080, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 103 Loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 104 Loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 105 Loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 106 Loss: tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 107 Loss: tensor(0.0309, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 108 Loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 109 Loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 110 Loss: tensor(0.0064, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 111 Loss: tensor(0.0094, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 112 Loss: tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 113 Loss: tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 114 Loss: tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 115 Loss: tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 116 Loss: tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 117 Loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 118 Loss: tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 119 Loss: tensor(0.0079, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 120 Loss: tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 121 Loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 122 Loss: tensor(0.0048, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 123 Loss: tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 124 Loss: tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 125 Loss: tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 126 Loss: tensor(0.0308, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 127 Loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 128 Loss: tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 129 Loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 130 Loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 131 Loss: tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 132 Loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 133 Loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 134 Loss: tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 135 Loss: tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 136 Loss: tensor(0.0080, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 137 Loss: tensor(0.0168, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 138 Loss: tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 139 Loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 140 Loss: tensor(0.0047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 141 Loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 142 Loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 143 Loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 144 Loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 145 Loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 146 Loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 147 Loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 148 Loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 149 Loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 150 Loss: tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 151 Loss: tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 152 Loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 153 Loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 154 Loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 155 Loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 156 Loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 157 Loss: tensor(0.0066, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 158 Loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 159 Loss: tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 160 Loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 161 Loss: tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m low_res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(low_res)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m posterior_initial \u001b[38;5;241m=\u001b[39m sample_p_0()\n\u001b[0;32m---> 31\u001b[0m posterior_final \u001b[38;5;241m=\u001b[39m \u001b[43mula_posterior_preconditioner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposterior_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_high\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optG\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# observation = torch.spmm(H,posterior_final.reshape(N_high*N_high,1)).reshape(1,N_low,N_low)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 30\u001b[0m, in \u001b[0;36mula_posterior_preconditioner\u001b[0;34m(z, b_high, x, G)\u001b[0m\n\u001b[1;32m     28\u001b[0m downscaled \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(z\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,N_high,N_high),(N_low,N_low))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,N_low,N_low)\n\u001b[1;32m     29\u001b[0m x_hat \u001b[38;5;241m=\u001b[39m downscaled \u001b[38;5;241m+\u001b[39m G(x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,N_low,N_low))\u001b[38;5;241m.\u001b[39mreshape(N_low,N_low)\n\u001b[0;32m---> 30\u001b[0m log_likelihood \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mll_sigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx_hat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mN_low\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx_hat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_low\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     31\u001b[0m grad_ll \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(log_likelihood, z)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Grad prior\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train with sampled data\n",
    "epoch_num = 1000\n",
    "lr = 0.001\n",
    "gamma = 0.1\n",
    "step_size = 50\n",
    "minimum_loss = float('inf')\n",
    "loss_track = []\n",
    "\n",
    "K = 4000\n",
    "s = 0.0004\n",
    "\n",
    "G = ResidualLearning()\n",
    "G.apply(weights_init_xavier).to(device)\n",
    "mse = nn.MSELoss(reduction='sum')\n",
    "optG = torch.optim.Adam(G.parameters(), lr = lr, weight_decay=0, betas=(0.5, 0.999))\n",
    "r_scheduleG = torch.optim.lr_scheduler.StepLR(optG, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Logger info\n",
    "dir_name = f'models/model3/21_81/lr{lr}_gamma{gamma}_stepsize{step_size}_K{K}'\n",
    "makedir(dir_name)\n",
    "logger = setup_logging('job0', dir_name, console=True)\n",
    "logger.info(f'Training for {epoch_num} epoches and learning rate is {lr}')\n",
    "\n",
    "for epoch in range(1, epoch_num+1):\n",
    "    \n",
    "    b_high, low_res = sample_data()\n",
    "    b_high = torch.tensor(b_high).to(torch.float32).to(device)\n",
    "    low_res = torch.tensor(low_res).to(torch.float32).to(device)\n",
    "    \n",
    "    posterior_initial = sample_p_0()\n",
    "    posterior_final = ula_posterior_preconditioner(posterior_initial, b_high, low_res, G)\n",
    "\n",
    "    optG.zero_grad()\n",
    "    \n",
    "    observation = torch.spmm(H,posterior_final.reshape(N_high*N_high,1)).reshape(1,N_low,N_low)\n",
    "    out = G(observation.reshape(1,N_low,N_low)) + observation\n",
    "    loss = mse(out,low_res.reshape(1,N_low,N_low))\n",
    "        \n",
    "    loss.backward()\n",
    "    optG.step()\n",
    "    \n",
    "    if loss < minimum_loss:\n",
    "        save_model(dir_name, epoch, 'best_model', r_scheduleG, G, optG)\n",
    "        minimum_loss = loss\n",
    "            \n",
    "    if epoch%100 == 0:\n",
    "        save_model(dir_name, epoch, 'model_epoch_{}'.format(epoch), r_scheduleG, G, optG)\n",
    "    \n",
    "    save_model(dir_name, epoch, 'current_epoch', r_scheduleG, G, optG)\n",
    "    \n",
    "    loss_track.append(loss.cpu().data.numpy())\n",
    "    np.save(f'{dir_name}/chains/loss_curve.npy', np.array(loss_track))\n",
    "    \n",
    "    print(\"Epoch:\", epoch, \"Loss:\", loss)\n",
    "\n",
    "    r_scheduleG.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upscale By 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_low = 21\n",
    "N_high = 121\n",
    "scale = 6\n",
    "a,b,c = 8,5,5\n",
    "\n",
    "h_low = 1/(N_low-1)\n",
    "x_low = np.arange(0,1.0001,h_low)\n",
    "y_low = np.arange(0,1.0001,h_low)\n",
    "\n",
    "h_high = 1/(N_high-1)\n",
    "x_high = np.arange(0,1.0001,h_high)\n",
    "y_high = np.arange(0,1.0001,h_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_high = create_A(N_high)\n",
    "A_low = create_A(N_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Parameters for RBF kernal\n",
    "prior_sigma = 0.002\n",
    "ll_sigma = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.eye(N_high**2) * prior_sigma**2\n",
    "G_inverse = np.eye(N_high**2) * (1/prior_sigma**2)\n",
    "\n",
    "# Code downscaling matrix\n",
    "H = np.zeros((N_low*N_low, N_high*N_high))\n",
    "\n",
    "submatrix = np.zeros((N_low,N_high))\n",
    "for i in range(N_low):\n",
    "    submatrix[i,scale*i] = 1\n",
    "    \n",
    "for j in range(N_low):\n",
    "    H[N_low*j:N_low*(j+1),N_high*scale*j:N_high*(scale*j+1)] = submatrix\n",
    "    \n",
    "# Turn matrices to tensors\n",
    "G = torch.tensor(G).to(torch.float32).to(device)\n",
    "G_inverse = torch.tensor(G_inverse).to(torch.float32).to(device)\n",
    "H = torch.tensor(H).to(torch.float32).to(device)\n",
    "A_high = torch.tensor(create_A(N_high)).to(torch.float32).to(device)\n",
    "b_high = torch.tensor(create_forcing_term(N_high,a,b,c)).to(torch.float32).to(device)\n",
    "\n",
    "# Store sparse matrices as sparse tensor\n",
    "A_high = A_high.to_sparse()\n",
    "H = H.to_sparse()\n",
    "G = G.to_sparse()\n",
    "G_inverse = G_inverse.to_sparse()\n",
    "operator = torch.spmm(A_high.T,G_inverse).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataFromH5File4(\"/home/pz281@ad.eng.cam.ac.uk/mnt/PhD/Pro_Down_SR/data/21_121_low_forcing.h5\")\n",
    "\n",
    "trainset = random.sample(range(0, 128), 100)\n",
    "testset = [i for i in range(0,128) if i not in trainset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data():\n",
    "    coefficient = random.sample(trainset,1)[0]\n",
    "    forcing = dataset[coefficient][0]\n",
    "    lr = dataset[coefficient][1]\n",
    "    \n",
    "    return forcing, lr\n",
    "\n",
    "\n",
    "def sample_p_0():\n",
    "    # Randomly sampling for initialisation of the Langevin dynamics\n",
    "    # prior = torch.randn(*[batch_size,1,20,20]).to(device)\n",
    "    \n",
    "    # Set the u_low_mean to the initialisation of the Langevin dynamics\n",
    "    posterior_initial = torch.randn([N_high,N_high]).to(torch.float32)\n",
    "    posterior_initial = torch.tensor(posterior_initial).to(device).to(torch.float32)\n",
    "    \n",
    "    return posterior_initial\n",
    "\n",
    "    \n",
    "def ula_posterior_preconditioner(z, b_high, x, G):\n",
    "    \"\"\"\n",
    "    Langevin dynamics with preconditioner\n",
    "    \"\"\"\n",
    "    z = z.clone().detach().requires_grad_(True)\n",
    "    for i in range(K):\n",
    "        # Grad log-likelihood\n",
    "        observation = torch.spmm(H,z.reshape(N_high*N_high,1)).reshape(N_low,N_low)\n",
    "        x_hat = observation + G(observation.reshape(1,N_low,N_low)).reshape(N_low,N_low)\n",
    "        log_likelihood = (-1/(2*math.pow(ll_sigma, 2)) * torch.matmul((x-x_hat).reshape(1,N_low**2),(x-x_hat).reshape(N_low**2,1)))\n",
    "        grad_ll = torch.autograd.grad(log_likelihood, z)[0]\n",
    "\n",
    "        # Grad prior\n",
    "        difference = torch.spmm(A_high,z.reshape(N_high*N_high,1)) - b_high.reshape(N_high**2,1)\n",
    "        # log_prior = - 0.5 * difference.T @ G_inverse @ difference\n",
    "        # grad_log_prior = torch.autograd.grad(log_prior, z)[0]\n",
    "        grad_log_prior = (- torch.spmm(operator,difference)).reshape(N_high,N_high)\n",
    "        \n",
    "        # Random noise term\n",
    "        W = torch.randn(*[N_high,N_high]).to(device)\n",
    "        # random = torch.matmul(G_sqrt,W.reshape(N_high**2,1)).reshape(N_high,N_high)\n",
    "        \n",
    "        z = z + 0.5 * s ** 2 * grad_log_prior + 0.5 * s ** 2 * grad_ll + s * W\n",
    "        # chains_evolution.append(z.cpu().data.numpy())   \n",
    "           \n",
    "    return z.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory already exists\n",
      "2024-06-11 13:39:55,799 : Training for 1000 epoches and learning rate is 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11851/232675414.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  posterior_initial = torch.tensor(posterior_initial).to(device).to(torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: tensor(0.0383, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 2 Loss: tensor(0.0117, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 3 Loss: tensor(0.0051, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 4 Loss: tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 5 Loss: tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 6 Loss: tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 7 Loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 8 Loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 9 Loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 10 Loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 11 Loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 12 Loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 13 Loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 14 Loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 15 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 16 Loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 17 Loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 18 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 19 Loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 20 Loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 21 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 22 Loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 23 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 24 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 25 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 26 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 27 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 28 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 29 Loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 30 Loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 31 Loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 32 Loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 33 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 34 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 35 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 36 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 37 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 38 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 39 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 40 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 41 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 42 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 43 Loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 44 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 45 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 46 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 47 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 48 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 49 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 50 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 51 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 52 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 53 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 54 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 55 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 56 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 57 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 58 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 59 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 60 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 61 Loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 62 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 63 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 64 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 65 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 66 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 67 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 68 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 69 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 70 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 71 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 72 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 73 Loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 74 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 75 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 76 Loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 77 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 78 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 79 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 80 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 81 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 82 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 83 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 84 Loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 85 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 86 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 87 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 88 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 89 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 90 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 91 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 92 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 93 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 94 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 95 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 96 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 97 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 98 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 99 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 100 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 101 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 102 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 103 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 104 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 105 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 106 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 107 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 108 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 109 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 110 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 111 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 112 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 113 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 114 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 115 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 116 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 117 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 118 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 119 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 120 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 121 Loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 122 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 123 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 124 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 125 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 126 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 127 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 128 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 129 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 130 Loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 131 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 132 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 133 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 134 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 135 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 136 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 137 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 138 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 139 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 140 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 141 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 142 Loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 143 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 144 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 145 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 146 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 147 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 148 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 149 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 150 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 151 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 152 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 153 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 154 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 155 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 156 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 157 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 158 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 159 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 160 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 161 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 162 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 163 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 164 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 165 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 166 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 167 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 168 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 169 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 170 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 171 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 172 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 173 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 174 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 175 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 176 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 177 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 178 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 179 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 180 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 181 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 182 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 183 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 184 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 185 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 186 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 187 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 188 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 189 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 190 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 191 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 192 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 193 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 194 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 195 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 196 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 197 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 198 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 199 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 200 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 201 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 202 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 203 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 204 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 205 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 206 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 207 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 208 Loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 209 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 210 Loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m low_res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(low_res)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m posterior_initial \u001b[38;5;241m=\u001b[39m sample_p_0()\n\u001b[0;32m---> 31\u001b[0m posterior_final \u001b[38;5;241m=\u001b[39m \u001b[43mula_posterior_preconditioner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposterior_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_high\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optG\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m observation \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mspmm(H,posterior_final\u001b[38;5;241m.\u001b[39mreshape(N_high\u001b[38;5;241m*\u001b[39mN_high,\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,N_low,N_low)\n",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m, in \u001b[0;36mula_posterior_preconditioner\u001b[0;34m(z, b_high, x, G)\u001b[0m\n\u001b[1;32m     28\u001b[0m x_hat \u001b[38;5;241m=\u001b[39m observation \u001b[38;5;241m+\u001b[39m G(observation\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,N_low,N_low))\u001b[38;5;241m.\u001b[39mreshape(N_low,N_low)\n\u001b[1;32m     29\u001b[0m log_likelihood \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mmath\u001b[38;5;241m.\u001b[39mpow(ll_sigma, \u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul((x\u001b[38;5;241m-\u001b[39mx_hat)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,N_low\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m),(x\u001b[38;5;241m-\u001b[39mx_hat)\u001b[38;5;241m.\u001b[39mreshape(N_low\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m---> 30\u001b[0m grad_ll \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_likelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Grad prior\u001b[39;00m\n\u001b[1;32m     33\u001b[0m difference \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mspmm(A_high,z\u001b[38;5;241m.\u001b[39mreshape(N_high\u001b[38;5;241m*\u001b[39mN_high,\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m-\u001b[39m b_high\u001b[38;5;241m.\u001b[39mreshape(N_high\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.9/site-packages/torch/autograd/__init__.py:300\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train with sampled data\n",
    "epoch_num = 1000\n",
    "lr = 0.003\n",
    "gamma = 0.5\n",
    "step_size = 50\n",
    "minimum_loss = float('inf')\n",
    "loss_track = []\n",
    "\n",
    "K = 9000\n",
    "s = 0.0004\n",
    "\n",
    "G = ResidualLearning()\n",
    "G.apply(weights_init_xavier).to(device)\n",
    "mse = nn.MSELoss(reduction='sum')\n",
    "optG = torch.optim.Adam(G.parameters(), lr = lr, weight_decay=0, betas=(0.5, 0.999))\n",
    "r_scheduleG = torch.optim.lr_scheduler.StepLR(optG, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Logger info\n",
    "dir_name = f'models/model3/21_121/2_lr{lr}_gamma{gamma}_stepsize{step_size}_K{K}_llsigma_{ll_sigma}_psigma_{prior_sigma}'\n",
    "makedir(dir_name)\n",
    "logger = setup_logging('job0', dir_name, console=True)\n",
    "logger.info(f'Training for {epoch_num} epoches and learning rate is {lr}')\n",
    "\n",
    "for epoch in range(1, epoch_num+1):\n",
    "    \n",
    "    b_high, low_res = sample_data()\n",
    "    b_high = torch.tensor(b_high).to(torch.float32).to(device)\n",
    "    low_res = torch.tensor(low_res).to(torch.float32).to(device)\n",
    "    \n",
    "    posterior_initial = sample_p_0()\n",
    "    posterior_final = ula_posterior_preconditioner(posterior_initial, b_high, low_res, G)\n",
    "\n",
    "    optG.zero_grad()\n",
    "    \n",
    "    observation = torch.spmm(H,posterior_final.reshape(N_high*N_high,1)).reshape(1,N_low,N_low)\n",
    "    residual = low_res.reshape(1,N_low,N_low) - observation\n",
    "    out = G(observation.reshape(1,N_low,N_low))\n",
    "    loss = mse(out,residual)\n",
    "        \n",
    "    loss.backward()\n",
    "    optG.step()\n",
    "    \n",
    "    if loss < minimum_loss:\n",
    "        save_model(dir_name, epoch, 'best_model', r_scheduleG, G, optG)\n",
    "        minimum_loss = loss\n",
    "            \n",
    "    if epoch%100 == 0:\n",
    "        save_model(dir_name, epoch, 'model_epoch_{}'.format(epoch), r_scheduleG, G, optG)\n",
    "    \n",
    "    save_model(dir_name, epoch, 'current_epoch', r_scheduleG, G, optG)\n",
    "    \n",
    "    loss_track.append(loss.cpu().data.numpy())\n",
    "    np.save(f'{dir_name}/chains/loss_curve.npy', np.array(loss_track))\n",
    "    \n",
    "    print(\"Epoch:\", epoch, \"Loss:\", loss)\n",
    "\n",
    "    r_scheduleG.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
