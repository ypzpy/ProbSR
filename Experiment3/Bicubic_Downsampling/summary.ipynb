{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS']='2'\n",
    "os.environ['LD_LIBRARY_PATH']=''\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pengyu.zhang/project/superres/ProbSR/Experiment2\n"
     ]
    }
   ],
   "source": [
    "%cd /home/pengyu.zhang/project/superres/ProbSR/Experiment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generation import *\n",
    "from scipy.linalg import sqrtm\n",
    "from downscaling import *\n",
    "from utils import *\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pengyu.zhang/project/superres/ProbSR/Experiment2/Bicubic_Downsampling\n"
     ]
    }
   ],
   "source": [
    "%cd Bicubic_Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_low = 40\n",
    "N_high = 160\n",
    "scale = 4\n",
    "a,b,c,d = -2.5,-2.5,1,0\n",
    "\n",
    "h_low = 1/(N_low-1)\n",
    "x_low = np.arange(0,1.0001,h_low)\n",
    "y_low = np.arange(0,1.0001,h_low)\n",
    "\n",
    "h_high = 1/(N_high-1)\n",
    "x_high = np.arange(0,1.0001,h_high)\n",
    "y_high = np.arange(0,1.0001,h_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_low, r_low, A_low, x_low, y_low = generate_data(N_low,a,b,c,d)\n",
    "w_high, r_high, A_high, x_high, y_high = generate_data(N_high,a,b,c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Parameters for prior variance\n",
    "prior_sigma = 0.002\n",
    "ll_sigma = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_high = csr_matrix(A_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator = (A_high.T) * (1/prior_sigma**2)\n",
    "b_high = create_forcing_term(N_high,a,b,c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G1 = DownScale().to(device)\n",
    "G1.load_state_dict(torch.load('/home/pengyu.zhang/project/superres/ProbSR/Experiment2/Bicubic_Downsampling/models/model1/40_160/lr0.003_gamma0.5_stepsize30_K70_llsigma_0.002_psigma0.002/ckpt/current_epoch.pth')['netG'])\n",
    "\n",
    "G2 = DownScale().to(device)\n",
    "G2.load_state_dict(torch.load('/home/pengyu.zhang/project/superres/ProbSR/Experiment2/Bicubic_Downsampling/models/model2/40_160/lr0.003_gamma0.5_stepsize50_K80_llsigma_0.002_psigma0.002/ckpt/current_epoch.pth')['netG'])\n",
    "\n",
    "G3 = U_net(1,1).to(device)\n",
    "G3.load_state_dict(torch.load('/home/pengyu.zhang/project/superres/ProbSR/Experiment2/Bicubic_Downsampling/models/model3/40_160_unet/lr0.001_gamma0.5_stepsize100_K100_llsigma_0.002_psigma_0.002/ckpt/current_epoch.pth')['netG'])\n",
    "\n",
    "#G4 = ResidualLearning().to(device)\n",
    "#G4.load_state_dict(torch.load('/home/pengyu.zhang/project/superres/ProbSR/Experiment1/Bicubic_Downsampling/models/after_pre_train/model3_8samples/40_160/lr0.0001_gamma0.5_stepsize30_K50_llsigma_0.001_psigma_0.002/ckpt/current_epoch.pth')['netG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04701015328452186 0.045361869575345375 0.09538205784634635 0.04199612253454898 0.046763187019127066\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d = -2.5,-2.5,1,0\n",
    "w_low, r_low, A_low, x_low, y_low = generate_data(N_low,a,b,c,d)\n",
    "w_high, r_high, A_high, x_high, y_high = generate_data(N_high,a,b,c,d)\n",
    "b_high = create_forcing_term(N_high,a,b,c,d)\n",
    "\n",
    "\n",
    "# Direct bicubic\n",
    "x = torch.tensor(w_low).to(torch.float32)\n",
    "w_sr = F.interpolate(x.reshape(1,1,N_low,N_low),(N_high,N_high),mode=\"bicubic\").reshape(N_high,N_high)\n",
    "error1 = abs(w_high - w_sr.cpu().data.numpy())\n",
    "error1 = (error1**2).sum()/error1.shape[0]**2\n",
    "\n",
    "\n",
    "# Parameters for Langevin dynamics\n",
    "K = 50\n",
    "s = 0.0004\n",
    "\n",
    "\n",
    "# Bicubic downsampling\n",
    "x = torch.tensor(w_low).to(torch.float32)\n",
    "z = F.interpolate(x.reshape(1,1,N_low,N_low),(N_high,N_high),mode=\"bicubic\").reshape(N_high,N_high)\n",
    "z = z.clone().detach().requires_grad_(True)\n",
    "chains_evolution = []\n",
    "\n",
    "for i in range(K):\n",
    "    # Grad log-likelihood\n",
    "    x_hat = F.interpolate(z.reshape(1,1,N_high,N_high),(N_low,N_low),mode=\"bicubic\").reshape(N_low,N_low)\n",
    "    log_likelihood = (-1/(2*math.pow(ll_sigma, 2)) * torch.matmul((x-x_hat).reshape(1,N_low**2),(x-x_hat).reshape(N_low**2,1)))\n",
    "    grad_ll = torch.autograd.grad(log_likelihood, z)[0]\n",
    "\n",
    "    # startTime = time.time()\n",
    "    # Grad prior\n",
    "    difference = A_high.dot(z.cpu().data.numpy().reshape(N_high*N_high,1)) - b_high.reshape(N_high**2,1)\n",
    "    # log_prior = - 0.5 * difference.T @ G_inverse @ difference\n",
    "    # grad_log_prior = torch.autograd.grad(log_prior, z)[0]\n",
    "    grad_log_prior = (- operator.dot(difference)).reshape(N_high,N_high)\n",
    "    grad_log_prior = torch.tensor(grad_log_prior).to(torch.float32)\n",
    "    #endTime = time.time()\n",
    "    #print(endTime-startTime)\n",
    "    # grad_log_likelihood = torch.matmul(G,grad_ll.reshape(N_high**2,1)).reshape(N_high,N_high)\n",
    "    \n",
    "    # Random noise term\n",
    "    W = torch.randn(*[N_high,N_high])\n",
    "    # random = torch.matmul(G_sqrt,W.reshape(N_high**2,1)).reshape(N_high,N_high)\n",
    "    \n",
    "    z = z + 0.5 * s ** 2 * grad_log_prior + 0.5 * s ** 2 * grad_ll + s * W\n",
    "\n",
    "    if i > K-10:\n",
    "        chains_evolution.append(z.cpu().data.numpy()) \n",
    "\n",
    "w_sr_bicubic = np.mean(chains_evolution,axis=0)\n",
    "error2 = abs(w_high - w_sr_bicubic)\n",
    "error2 = (error2**2).sum()/error2.shape[0]**2\n",
    "\n",
    "\n",
    "# Model 1\n",
    "\n",
    "x = torch.tensor(w_low).to(torch.float32).to(device)\n",
    "z = F.interpolate(x.reshape(1,1,N_low,N_low),(N_high,N_high),mode=\"bicubic\").reshape(N_high,N_high)\n",
    "z = z.clone().detach().requires_grad_(True)\n",
    "chains_evolution = []\n",
    "\n",
    "for i in range(K):\n",
    "    # Grad log-likelihood\n",
    "    x_hat = G1(z.reshape(1,N_high,N_high)).reshape(N_low,N_low)\n",
    "    log_likelihood = (-1/(2*math.pow(ll_sigma, 2)) * torch.matmul((x-x_hat).reshape(1,N_low**2),(x-x_hat).reshape(N_low**2,1)))\n",
    "    grad_ll = torch.autograd.grad(log_likelihood, z)[0]\n",
    "    # grad_log_likelihood = torch.matmul(G,grad_ll.reshape(N_high**2,1)).reshape(N_high,N_high)\n",
    "    \n",
    "    # Grad prior\n",
    "    difference = A_high.dot(z.cpu().data.numpy().reshape(N_high*N_high,1)) - b_high.reshape(N_high**2,1)\n",
    "    # log_prior = - 0.5 * difference.T @ G_inverse @ difference\n",
    "    # grad_log_prior = torch.autograd.grad(log_prior, z)[0]\n",
    "    grad_log_prior = (- operator.dot(difference)).reshape(N_high,N_high)\n",
    "    grad_log_prior = torch.tensor(grad_log_prior).to(torch.float32).to(device)\n",
    "    \n",
    "    # Random noise term\n",
    "    W = torch.randn(*[N_high,N_high]).to(device)\n",
    "    # random = torch.matmul(G_sqrt,W.reshape(N_high**2,1)).reshape(N_high,N_high)\n",
    "    \n",
    "    z = z + 0.5 * s ** 2 * grad_log_prior + 0.5 * s ** 2 * grad_ll + s * W\n",
    "\n",
    "    if i > K-10:\n",
    "        chains_evolution.append(z.cpu().data.numpy()) \n",
    "\n",
    "w_sr_model1 = np.mean(chains_evolution,axis=0)\n",
    "error3 = abs(w_high - w_sr_model1)\n",
    "error3 = (error3**2).sum()/error3.shape[0]**2\n",
    "\n",
    "\n",
    "# Model 2\n",
    "\n",
    "x = torch.tensor(w_low).to(torch.float32).to(device)\n",
    "z = F.interpolate(x.reshape(1,1,N_low,N_low),(N_high,N_high),mode=\"bicubic\").reshape(N_high,N_high)\n",
    "z = z.clone().detach().requires_grad_(True)\n",
    "chains_evolution = []\n",
    "\n",
    "for i in range(K):\n",
    "    # Grad log-likelihood\n",
    "    downscaled = F.interpolate(z.reshape(1,1,N_high,N_high),(N_low,N_low)).reshape(N_low,N_low)\n",
    "    x_hat = downscaled + G2(z.reshape(1,1,N_high,N_high)).reshape(N_low,N_low)\n",
    "    log_likelihood = (-1/(2*math.pow(ll_sigma, 2)) * torch.matmul((x-x_hat).reshape(1,N_low**2),(x-x_hat).reshape(N_low**2,1)))\n",
    "    grad_ll = torch.autograd.grad(log_likelihood, z)[0]\n",
    "    # grad_log_likelihood = torch.matmul(G,grad_ll.reshape(N_high**2,1)).reshape(N_high,N_high)\n",
    "    \n",
    "    # Grad prior\n",
    "    difference = A_high.dot(z.cpu().data.numpy().reshape(N_high*N_high,1)) - b_high.reshape(N_high**2,1)\n",
    "    # log_prior = - 0.5 * difference.T @ G_inverse @ difference\n",
    "    # grad_log_prior = torch.autograd.grad(log_prior, z)[0]\n",
    "    grad_log_prior = (- operator.dot(difference)).reshape(N_high,N_high)\n",
    "    grad_log_prior = torch.tensor(grad_log_prior).to(torch.float32).to(device)\n",
    "    \n",
    "    # Random noise term\n",
    "    W = torch.randn(*[N_high,N_high]).to(device)\n",
    "    # random = torch.matmul(G_sqrt,W.reshape(N_high**2,1)).reshape(N_high,N_high)\n",
    "    \n",
    "    z = z + 0.5 * s ** 2 * grad_log_prior + 0.5 * s ** 2 * grad_ll + s * W\n",
    "\n",
    "    if i > K-10:\n",
    "        chains_evolution.append(z.cpu().data.numpy()) \n",
    "\n",
    "w_sr_model2 = np.mean(chains_evolution,axis=0)\n",
    "error4 = abs(w_high - w_sr_model2)\n",
    "error4 = (error4**2).sum()/error4.shape[0]**2\n",
    "\n",
    "\n",
    "\n",
    "# Model 3\n",
    "\n",
    "x = torch.tensor(w_low).to(torch.float32).to(device)\n",
    "z = F.interpolate(x.reshape(1,1,N_low,N_low),(N_high,N_high),mode=\"bicubic\").reshape(N_high,N_high)\n",
    "z = z.clone().detach().requires_grad_(True)\n",
    "chains_evolution = []\n",
    "\n",
    "for i in range(K):\n",
    "    # Grad log-likelihood\n",
    "    downscaled = F.interpolate(z.reshape(1,1,N_high,N_high),(N_low,N_low)).reshape(N_low,N_low)\n",
    "    x_hat = downscaled + G3(downscaled.reshape(1,1,N_low,N_low)).reshape(N_low,N_low)\n",
    "    log_likelihood = (-1/(2*math.pow(ll_sigma, 2)) * torch.matmul((x-x_hat).reshape(1,N_low**2),(x-x_hat).reshape(N_low**2,1)))\n",
    "    grad_ll = torch.autograd.grad(log_likelihood, z)[0]\n",
    "    # grad_log_likelihood = torch.matmul(G,grad_ll.reshape(N_high**2,1)).reshape(N_high,N_high)\n",
    "    \n",
    "    # Grad prior\n",
    "    difference = A_high.dot(z.cpu().data.numpy().reshape(N_high*N_high,1)) - b_high.reshape(N_high**2,1)\n",
    "    # log_prior = - 0.5 * difference.T @ G_inverse @ difference\n",
    "    # grad_log_prior = torch.autograd.grad(log_prior, z)[0]\n",
    "    grad_log_prior = (- operator.dot(difference)).reshape(N_high,N_high)\n",
    "    grad_log_prior = torch.tensor(grad_log_prior).to(torch.float32).to(device)\n",
    "    \n",
    "    # Random noise term\n",
    "    W = torch.randn(*[N_high,N_high]).to(device)\n",
    "    # random = torch.matmul(G_sqrt,W.reshape(N_high**2,1)).reshape(N_high,N_high)\n",
    "    \n",
    "    z = z + 0.5 * s ** 2 * grad_log_prior + 0.5 * s ** 2 * grad_ll + s * W\n",
    "    \n",
    "    if i > K-10:\n",
    "        chains_evolution.append(z.cpu().data.numpy()) \n",
    "\n",
    "w_sr_model3 = np.mean(chains_evolution,axis=0)\n",
    "error5 = abs(w_high - w_sr_model3)\n",
    "error5 = (error5**2).sum()/error5.shape[0]**2\n",
    "\n",
    "\n",
    "print(error1,error2,error3,error4,error5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
