{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS']='2'\n",
    "os.environ['LD_LIBRARY_PATH']=''\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pengyu.zhang/project/superres/ProbSR/Experiment3\n"
     ]
    }
   ],
   "source": [
    "%cd /home/pengyu.zhang/project/superres/ProbSR/Experiment3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generation import *\n",
    "from scipy.linalg import sqrtm\n",
    "from downscaling import *\n",
    "from utils import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pengyu.zhang/project/superres/ProbSR/Experiment3/Bicubic_Downsampling\n"
     ]
    }
   ],
   "source": [
    "%cd Bicubic_Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langevin & Training Downscale Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upscale By 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_low = 16\n",
    "N_high = 64\n",
    "scale = 4\n",
    "a, b, c, d = 1,1,1,0\n",
    "\n",
    "h_low = 1/(N_low-1)\n",
    "x_low = np.arange(0,1.0001,h_low)\n",
    "y_low = np.arange(0,1.0001,h_low)\n",
    "z_low = np.arange(0,1.0001,h_low)\n",
    "\n",
    "h_high = 1/(N_high-1)\n",
    "x_high = np.arange(0,1.0001,h_high)\n",
    "y_high = np.arange(0,1.0001,h_high)\n",
    "z_high = np.arange(0,1.0001,h_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_high = create_A(N_high)\n",
    "A_low = create_A(N_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Parameters for prior variance\n",
    "prior_sigma = 0.0001\n",
    "ll_sigma = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch_tensor(A):\n",
    "    coo = A.tocoo()\n",
    "    values = coo.data\n",
    "    indices = np.vstack((coo.row,coo.col))\n",
    "\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = coo.shape\n",
    "\n",
    "    sparse_tensor = torch.sparse.FloatTensor(i,v,torch.Size(shape))\n",
    "\n",
    "    return sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3576054/2037511837.py:10: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)\n",
      "  sparse_tensor = torch.sparse.FloatTensor(i,v,torch.Size(shape))\n"
     ]
    }
   ],
   "source": [
    "A_high = to_torch_tensor(A_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_high = A_high.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator = (A_high.T) * (1/prior_sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataFromH5File4(\"/home/pengyu.zhang/project/superres/ProbSR/Experiment3/data/16_64_low_forcing.h5\")\n",
    "trainset = random.sample(range(0, 1000), 800)\n",
    "testset = [i for i in range(0,1000) if i not in trainset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data():\n",
    "    coefficient = random.sample(trainset,1)[0]\n",
    "    forcing = dataset[coefficient][0]\n",
    "    lr = dataset[coefficient][1]\n",
    "    \n",
    "    return forcing, lr\n",
    "\n",
    "\n",
    "def sample_p_0(x):\n",
    "    # Randomly sampling for initialisation of the Langevin dynamics\n",
    "    # prior = torch.randn(*[batch_size,1,20,20]).to(device)\n",
    "    \n",
    "    # Set the u_low_mean to the initialisation of the Langevin dynamics\n",
    "    # posterior_initial = torch.randn([N_high,N_high]).to(torch.float32)\n",
    "    # posterior_initial = torch.tensor(posterior_initial).to(device).to(torch.float32)\n",
    "    posterior_initial  = F.interpolate(x.reshape(1,1,N_low,N_low,N_low),(N_high,N_high,N_high),mode='trilinear').reshape(N_high,N_high,N_high)\n",
    "    \n",
    "    return posterior_initial\n",
    "\n",
    "    \n",
    "def ula_posterior_preconditioner(z, b_high, x, G):\n",
    "    \"\"\"\n",
    "    Langevin dynamics with preconditioner\n",
    "    \"\"\"\n",
    "    z = z.clone().detach().requires_grad_(True)\n",
    "    sum = 0\n",
    "    for i in range(K):\n",
    "        # Grad log-likelihood\n",
    "        downscaled = F.interpolate(z.reshape(1,1,N_high,N_high,N_high),(N_low,N_low,N_low)).reshape(N_low,N_low,N_low)\n",
    "        x_hat = downscaled + G(downscaled.reshape(1,1,N_low,N_low,N_low)).reshape(N_low,N_low,N_low)\n",
    "        log_likelihood = (-1/(2*math.pow(ll_sigma, 2)) * torch.matmul((x-x_hat).reshape(1,N_low**3),(x-x_hat).reshape(N_low**3,1)))\n",
    "        grad_ll = torch.autograd.grad(log_likelihood, z)[0]\n",
    "\n",
    "        # Grad prior\n",
    "        difference = torch.spmm(A_high,z.reshape(N_high**3,1)) - b_high.reshape(N_high**3,1)\n",
    "        # log_prior = - 0.5 * difference.T @ G_inverse @ difference\n",
    "        # grad_log_prior = torch.autograd.grad(log_prior, z)[0]\n",
    "        grad_log_prior = (- torch.spmm(operator,difference)).reshape(N_high,N_high,N_high)\n",
    "        \n",
    "        # Random noise term\n",
    "        W = torch.randn(*[N_high,N_high,N_high]).to(device)\n",
    "        # random = torch.matmul(G_sqrt,W.reshape(N_high**2,1)).reshape(N_high,N_high)\n",
    "        \n",
    "        z = z + 0.5 * s ** 2 * grad_log_prior + 0.5 * s ** 2 * grad_ll + s * W\n",
    "        \n",
    "        if i >= K-10:\n",
    "            sum += z\n",
    "        \n",
    "    sum /= 10\n",
    "           \n",
    "    return sum.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-15 13:08:35,034 : Training for 3000 epoches and learning rate is 0.001\n",
      "Epoch: 1 Loss: tensor(224.9430, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 2 Loss: tensor(246.6185, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 3 Loss: tensor(993.3023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 4 Loss: tensor(825.2459, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 5 Loss: tensor(704.6351, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 6 Loss: tensor(81.1092, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 7 Loss: tensor(624.3228, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 8 Loss: tensor(112.2015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 9 Loss: tensor(650.2920, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 10 Loss: tensor(724.2444, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 11 Loss: tensor(633.8945, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 12 Loss: tensor(265.1878, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 13 Loss: tensor(441.9989, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 14 Loss: tensor(335.2667, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 15 Loss: tensor(108.1640, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 16 Loss: tensor(34.8559, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 17 Loss: tensor(57.5192, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 18 Loss: tensor(31.7468, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 19 Loss: tensor(32.6478, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 20 Loss: tensor(19.8280, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 21 Loss: tensor(16.2731, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 22 Loss: tensor(22.7905, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 23 Loss: tensor(23.7759, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 24 Loss: tensor(13.7721, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 25 Loss: tensor(9.2261, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 26 Loss: tensor(16.3341, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 27 Loss: tensor(12.6204, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 28 Loss: tensor(8.6036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 29 Loss: tensor(13.9198, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 30 Loss: tensor(7.5138, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 31 Loss: tensor(9.7945, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 32 Loss: tensor(16.4049, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 33 Loss: tensor(8.0329, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 34 Loss: tensor(8.1514, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 35 Loss: tensor(9.8056, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 36 Loss: tensor(6.2355, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 37 Loss: tensor(6.9598, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 38 Loss: tensor(4.2984, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 39 Loss: tensor(4.2688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 40 Loss: tensor(5.2307, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 41 Loss: tensor(6.4612, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 42 Loss: tensor(4.6090, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 43 Loss: tensor(9.0054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 44 Loss: tensor(6.7891, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 45 Loss: tensor(8.1041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 46 Loss: tensor(5.3322, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 47 Loss: tensor(3.6891, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 48 Loss: tensor(6.1014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 49 Loss: tensor(7.6988, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 50 Loss: tensor(7.7911, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 51 Loss: tensor(6.5793, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 52 Loss: tensor(3.5777, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 53 Loss: tensor(6.6159, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 54 Loss: tensor(2.7536, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 55 Loss: tensor(4.1777, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 56 Loss: tensor(2.5932, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 57 Loss: tensor(2.5763, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 58 Loss: tensor(4.0362, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 59 Loss: tensor(3.4607, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 60 Loss: tensor(3.2838, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 61 Loss: tensor(6.2503, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 62 Loss: tensor(41.7915, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 63 Loss: tensor(4.4512, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 64 Loss: tensor(35.8768, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 65 Loss: tensor(3.5167, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 66 Loss: tensor(4.7834, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 67 Loss: tensor(5.6063, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 68 Loss: tensor(4.7021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 69 Loss: tensor(2.7173, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 70 Loss: tensor(3.6360, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 71 Loss: tensor(7.1340, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 72 Loss: tensor(4.1786, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 73 Loss: tensor(5.8404, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 74 Loss: tensor(3.9918, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 75 Loss: tensor(1.9254, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 76 Loss: tensor(2.3691, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 77 Loss: tensor(7.5091, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 78 Loss: tensor(2.9449, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 79 Loss: tensor(1.6688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 80 Loss: tensor(4.0186, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 81 Loss: tensor(3.2744, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 82 Loss: tensor(1.6145, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 83 Loss: tensor(8.8446, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 84 Loss: tensor(2.6341, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 85 Loss: tensor(2.7272, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 86 Loss: tensor(2.0933, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 87 Loss: tensor(1.5049, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 88 Loss: tensor(1.4819, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 89 Loss: tensor(1.3117, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 90 Loss: tensor(1.5870, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 91 Loss: tensor(1.1216, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 92 Loss: tensor(1.3929, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 93 Loss: tensor(1.9227, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 94 Loss: tensor(1.4212, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 95 Loss: tensor(2.6258, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 96 Loss: tensor(1.7539, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 97 Loss: tensor(3.4914, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 98 Loss: tensor(2.3638, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 99 Loss: tensor(4.9627, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 100 Loss: tensor(1.5777, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 101 Loss: tensor(1.6804, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 102 Loss: tensor(8.6318, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 103 Loss: tensor(1.3289, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 104 Loss: tensor(1.2705, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 105 Loss: tensor(1.6527, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 106 Loss: tensor(1.1483, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 107 Loss: tensor(8.8346, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 108 Loss: tensor(1.4267, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 109 Loss: tensor(2.1392, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 110 Loss: tensor(1.3495, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 111 Loss: tensor(1.5800, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 112 Loss: tensor(1.5724, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 113 Loss: tensor(1.6739, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 114 Loss: tensor(1.2679, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 115 Loss: tensor(1.2087, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 116 Loss: tensor(1.1600, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 117 Loss: tensor(2.3942, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 118 Loss: tensor(1.1083, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 119 Loss: tensor(7.4264, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 120 Loss: tensor(6.6489, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 121 Loss: tensor(2.1782, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 122 Loss: tensor(1.2794, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 123 Loss: tensor(7.5475, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 124 Loss: tensor(2.6128, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 125 Loss: tensor(1.7860, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 126 Loss: tensor(2.2429, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 127 Loss: tensor(1.1332, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 128 Loss: tensor(1.2338, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 129 Loss: tensor(1.2374, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 130 Loss: tensor(0.8758, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 131 Loss: tensor(2.7162, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 132 Loss: tensor(0.9598, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 133 Loss: tensor(1.3212, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 134 Loss: tensor(1.5921, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 135 Loss: tensor(1.6730, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 136 Loss: tensor(0.9331, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 137 Loss: tensor(1.9368, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 138 Loss: tensor(4.3395, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 139 Loss: tensor(1.1550, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 140 Loss: tensor(2.1871, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 141 Loss: tensor(1.6420, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 142 Loss: tensor(1.1758, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 143 Loss: tensor(1.5694, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 144 Loss: tensor(1.4345, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 145 Loss: tensor(1.2264, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 146 Loss: tensor(2.5771, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 147 Loss: tensor(1.1103, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 148 Loss: tensor(3.5673, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 149 Loss: tensor(1.1904, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 150 Loss: tensor(1.7142, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 151 Loss: tensor(1.0766, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 152 Loss: tensor(3.2345, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 153 Loss: tensor(1.1743, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 154 Loss: tensor(1.0918, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 155 Loss: tensor(1.6632, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 156 Loss: tensor(0.7047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 157 Loss: tensor(0.9034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 158 Loss: tensor(0.8105, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 159 Loss: tensor(0.9609, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 160 Loss: tensor(1.5172, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 161 Loss: tensor(0.8493, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 162 Loss: tensor(0.8350, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 163 Loss: tensor(1.9455, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 164 Loss: tensor(0.9080, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 165 Loss: tensor(0.6054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 166 Loss: tensor(0.8853, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 167 Loss: tensor(0.5369, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 168 Loss: tensor(1.2626, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 169 Loss: tensor(1.1131, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 170 Loss: tensor(0.7296, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 171 Loss: tensor(2.1074, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 172 Loss: tensor(0.9238, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 173 Loss: tensor(0.7563, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 174 Loss: tensor(1.1657, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 175 Loss: tensor(1.1028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 176 Loss: tensor(1.9603, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 177 Loss: tensor(1.4814, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 178 Loss: tensor(0.9882, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 179 Loss: tensor(0.9807, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 180 Loss: tensor(6.0447, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 181 Loss: tensor(0.9515, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 182 Loss: tensor(1.1822, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 183 Loss: tensor(4.1578, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 184 Loss: tensor(0.8600, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 185 Loss: tensor(1.5864, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 186 Loss: tensor(1.1215, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 187 Loss: tensor(3.4670, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 188 Loss: tensor(0.5450, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 189 Loss: tensor(1.1212, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 190 Loss: tensor(6.7385, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 191 Loss: tensor(1.5413, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 192 Loss: tensor(1.4489, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 193 Loss: tensor(1.1305, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 194 Loss: tensor(0.9640, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 195 Loss: tensor(0.8633, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 196 Loss: tensor(1.1414, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 197 Loss: tensor(0.8235, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 198 Loss: tensor(2.3798, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 199 Loss: tensor(0.5599, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 200 Loss: tensor(1.4082, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 201 Loss: tensor(1.6255, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 202 Loss: tensor(0.9107, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 203 Loss: tensor(0.6296, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 204 Loss: tensor(0.7827, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 205 Loss: tensor(0.8193, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 206 Loss: tensor(1.5304, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 207 Loss: tensor(1.0885, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 208 Loss: tensor(0.7919, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 209 Loss: tensor(1.1152, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 210 Loss: tensor(0.9386, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 211 Loss: tensor(0.7094, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 212 Loss: tensor(0.4596, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 213 Loss: tensor(22.4186, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 214 Loss: tensor(0.9881, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 215 Loss: tensor(2.3070, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 216 Loss: tensor(0.6912, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 217 Loss: tensor(1.4949, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 218 Loss: tensor(1.6340, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 219 Loss: tensor(1.2330, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 220 Loss: tensor(0.8095, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 221 Loss: tensor(0.6023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 222 Loss: tensor(0.8123, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 223 Loss: tensor(0.6620, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 224 Loss: tensor(1.5002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 225 Loss: tensor(0.7054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 226 Loss: tensor(8.1651, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 227 Loss: tensor(0.7055, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 228 Loss: tensor(1.2287, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 229 Loss: tensor(1.4236, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 230 Loss: tensor(1.1124, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 231 Loss: tensor(0.6346, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 232 Loss: tensor(0.7381, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 233 Loss: tensor(0.8451, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 234 Loss: tensor(16.1075, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 235 Loss: tensor(1.4465, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 236 Loss: tensor(0.6009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 237 Loss: tensor(1.0221, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 238 Loss: tensor(1.5324, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 239 Loss: tensor(0.6577, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 240 Loss: tensor(1.7345, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 241 Loss: tensor(0.5377, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 242 Loss: tensor(0.6183, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 243 Loss: tensor(3.3392, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 244 Loss: tensor(0.5516, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 245 Loss: tensor(0.5068, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 246 Loss: tensor(1.4328, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 247 Loss: tensor(0.5508, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 248 Loss: tensor(1.3100, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 249 Loss: tensor(0.8804, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 250 Loss: tensor(1.1857, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 251 Loss: tensor(0.5791, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 252 Loss: tensor(1.3711, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 253 Loss: tensor(1.8556, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 254 Loss: tensor(0.4800, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 255 Loss: tensor(1.9401, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 256 Loss: tensor(0.5976, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 257 Loss: tensor(0.5231, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 258 Loss: tensor(1.2263, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 259 Loss: tensor(1.0594, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 260 Loss: tensor(0.5847, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 261 Loss: tensor(0.4577, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 262 Loss: tensor(0.6685, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 263 Loss: tensor(0.8332, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 264 Loss: tensor(0.4808, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 265 Loss: tensor(1.0752, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 266 Loss: tensor(0.5641, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 267 Loss: tensor(1.4992, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 268 Loss: tensor(0.5662, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 269 Loss: tensor(3.3867, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 270 Loss: tensor(0.6035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 271 Loss: tensor(0.6437, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 272 Loss: tensor(0.5606, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 273 Loss: tensor(1.1679, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 274 Loss: tensor(0.6606, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 275 Loss: tensor(0.5165, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 276 Loss: tensor(0.5397, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 277 Loss: tensor(0.5101, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 278 Loss: tensor(0.7612, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 279 Loss: tensor(0.4717, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 280 Loss: tensor(0.5890, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 281 Loss: tensor(0.4197, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 282 Loss: tensor(0.7923, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 283 Loss: tensor(2.0670, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 284 Loss: tensor(0.4564, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 285 Loss: tensor(0.5645, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 286 Loss: tensor(0.4474, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 287 Loss: tensor(0.6432, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 288 Loss: tensor(1.1472, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 289 Loss: tensor(0.4310, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 290 Loss: tensor(0.4366, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 291 Loss: tensor(0.7965, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 292 Loss: tensor(0.5147, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 293 Loss: tensor(0.4643, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 294 Loss: tensor(1.1547, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 295 Loss: tensor(1.3226, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 296 Loss: tensor(0.8980, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 297 Loss: tensor(1.3302, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 298 Loss: tensor(0.4938, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 299 Loss: tensor(0.4571, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 300 Loss: tensor(2.4724, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 301 Loss: tensor(0.5102, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 302 Loss: tensor(2.4644, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 303 Loss: tensor(0.3622, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 304 Loss: tensor(0.4737, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 305 Loss: tensor(0.7999, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 306 Loss: tensor(0.9992, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 307 Loss: tensor(0.5619, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 308 Loss: tensor(0.8451, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 309 Loss: tensor(1.0820, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 310 Loss: tensor(0.6948, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 311 Loss: tensor(0.9360, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 312 Loss: tensor(2.5152, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 313 Loss: tensor(1.2351, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 314 Loss: tensor(0.4082, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 315 Loss: tensor(2.5504, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 316 Loss: tensor(0.9400, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 317 Loss: tensor(0.5083, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 318 Loss: tensor(1.7113, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 319 Loss: tensor(0.7461, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 320 Loss: tensor(1.3265, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 321 Loss: tensor(0.7471, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 322 Loss: tensor(1.8922, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 323 Loss: tensor(0.5683, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 324 Loss: tensor(0.5072, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 325 Loss: tensor(2.5896, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 326 Loss: tensor(2.2499, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 327 Loss: tensor(5.7862, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 328 Loss: tensor(0.6192, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 329 Loss: tensor(2.0081, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 330 Loss: tensor(0.8141, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 331 Loss: tensor(0.5099, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 332 Loss: tensor(0.8988, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 333 Loss: tensor(0.7290, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 334 Loss: tensor(0.7335, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 335 Loss: tensor(0.4421, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 336 Loss: tensor(0.8120, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 337 Loss: tensor(0.3977, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 338 Loss: tensor(0.4572, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 339 Loss: tensor(0.4014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 340 Loss: tensor(1.0358, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 341 Loss: tensor(0.9841, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 342 Loss: tensor(0.4487, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 343 Loss: tensor(0.6195, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 344 Loss: tensor(1.6229, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 345 Loss: tensor(0.4954, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 346 Loss: tensor(0.4855, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 347 Loss: tensor(0.5049, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 348 Loss: tensor(0.4980, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 349 Loss: tensor(0.9449, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 350 Loss: tensor(0.7254, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 351 Loss: tensor(0.3908, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 352 Loss: tensor(0.8693, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 353 Loss: tensor(0.5055, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 354 Loss: tensor(0.3563, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 355 Loss: tensor(0.5566, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 356 Loss: tensor(0.9936, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 357 Loss: tensor(2.2147, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 358 Loss: tensor(1.4393, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 359 Loss: tensor(0.3757, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 360 Loss: tensor(0.4114, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 361 Loss: tensor(0.5172, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 362 Loss: tensor(0.5544, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 363 Loss: tensor(0.7387, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 364 Loss: tensor(0.4433, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 365 Loss: tensor(1.5953, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 366 Loss: tensor(0.3803, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 367 Loss: tensor(0.4564, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 368 Loss: tensor(0.4071, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 369 Loss: tensor(0.6490, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 370 Loss: tensor(0.5071, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 371 Loss: tensor(1.6209, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 372 Loss: tensor(0.3447, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 373 Loss: tensor(0.4091, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 374 Loss: tensor(0.4415, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 375 Loss: tensor(0.5815, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 376 Loss: tensor(0.5086, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 377 Loss: tensor(0.3775, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 378 Loss: tensor(0.4213, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 379 Loss: tensor(0.5054, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 380 Loss: tensor(0.4223, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 381 Loss: tensor(0.5988, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 382 Loss: tensor(0.6580, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 383 Loss: tensor(0.7258, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 384 Loss: tensor(0.6109, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 385 Loss: tensor(0.5610, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 386 Loss: tensor(1.0781, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 387 Loss: tensor(2.1229, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 388 Loss: tensor(0.4794, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 389 Loss: tensor(0.8068, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 390 Loss: tensor(0.6249, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 391 Loss: tensor(1.8416, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 392 Loss: tensor(0.6422, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 393 Loss: tensor(0.7904, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 394 Loss: tensor(1.3309, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 395 Loss: tensor(0.5714, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 396 Loss: tensor(0.4805, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 397 Loss: tensor(0.8370, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 398 Loss: tensor(0.8005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 399 Loss: tensor(0.4503, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 400 Loss: tensor(1.3763, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 401 Loss: tensor(0.4935, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 402 Loss: tensor(0.3927, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 403 Loss: tensor(0.3905, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 404 Loss: tensor(0.6652, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 405 Loss: tensor(0.6402, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 406 Loss: tensor(1.0581, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 407 Loss: tensor(0.6975, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 408 Loss: tensor(0.3812, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 409 Loss: tensor(0.3518, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 410 Loss: tensor(0.5569, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 411 Loss: tensor(0.4267, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 412 Loss: tensor(0.5414, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 413 Loss: tensor(0.3816, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 414 Loss: tensor(0.4881, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 415 Loss: tensor(0.4414, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 416 Loss: tensor(0.4316, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 417 Loss: tensor(0.4643, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 418 Loss: tensor(0.6629, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 419 Loss: tensor(1.4696, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 420 Loss: tensor(0.5229, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 421 Loss: tensor(0.3442, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 422 Loss: tensor(0.8713, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 423 Loss: tensor(0.5227, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 424 Loss: tensor(0.3821, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 425 Loss: tensor(1.4047, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 426 Loss: tensor(0.3156, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 427 Loss: tensor(0.7094, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 428 Loss: tensor(0.3968, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 429 Loss: tensor(0.6345, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 430 Loss: tensor(1.0973, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 431 Loss: tensor(0.3665, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 432 Loss: tensor(1.8700, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 433 Loss: tensor(0.5571, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 434 Loss: tensor(0.5031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 435 Loss: tensor(1.3318, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 436 Loss: tensor(0.3962, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 437 Loss: tensor(0.3948, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 438 Loss: tensor(0.4616, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 439 Loss: tensor(0.5847, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 440 Loss: tensor(0.3860, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 441 Loss: tensor(0.3988, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 442 Loss: tensor(0.5148, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 443 Loss: tensor(0.3904, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 444 Loss: tensor(2.2348, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 445 Loss: tensor(0.5678, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 446 Loss: tensor(0.3525, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 447 Loss: tensor(0.4229, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 448 Loss: tensor(0.7170, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 449 Loss: tensor(0.4941, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 450 Loss: tensor(0.6950, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 451 Loss: tensor(0.3196, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 452 Loss: tensor(0.8039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 453 Loss: tensor(0.6163, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 454 Loss: tensor(0.7448, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 455 Loss: tensor(0.3489, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 456 Loss: tensor(0.3759, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 457 Loss: tensor(0.7600, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 458 Loss: tensor(0.5994, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 459 Loss: tensor(0.3663, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 460 Loss: tensor(0.5889, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 461 Loss: tensor(1.0780, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 462 Loss: tensor(1.9324, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 463 Loss: tensor(0.3690, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 464 Loss: tensor(0.3754, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 465 Loss: tensor(0.4097, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 466 Loss: tensor(0.3937, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 467 Loss: tensor(0.4227, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 468 Loss: tensor(0.3730, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 469 Loss: tensor(0.3296, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 470 Loss: tensor(0.5754, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 471 Loss: tensor(0.3708, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 472 Loss: tensor(3.0576, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 473 Loss: tensor(0.3442, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 474 Loss: tensor(2.2069, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 475 Loss: tensor(0.3527, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 476 Loss: tensor(0.3216, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 477 Loss: tensor(0.4645, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 478 Loss: tensor(0.6837, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 479 Loss: tensor(0.4455, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 480 Loss: tensor(1.0174, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 481 Loss: tensor(0.3517, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 482 Loss: tensor(0.4066, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m low_res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(low_res)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m posterior_initial \u001b[38;5;241m=\u001b[39m sample_p_0(low_res)\n\u001b[0;32m---> 34\u001b[0m posterior_final \u001b[38;5;241m=\u001b[39m \u001b[43mula_posterior_preconditioner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposterior_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_high\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m optG\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     38\u001b[0m downscaled \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(posterior_final\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,N_high,N_high,N_high),(N_low,N_low,N_low))\u001b[38;5;241m.\u001b[39mreshape(N_low,N_low,N_low)\n",
      "Cell \u001b[0;32mIn[14], line 41\u001b[0m, in \u001b[0;36mula_posterior_preconditioner\u001b[0;34m(z, b_high, x, G)\u001b[0m\n\u001b[1;32m     38\u001b[0m grad_log_prior \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mspmm(operator,difference))\u001b[38;5;241m.\u001b[39mreshape(N_high,N_high,N_high)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Random noise term\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mN_high\u001b[49m\u001b[43m,\u001b[49m\u001b[43mN_high\u001b[49m\u001b[43m,\u001b[49m\u001b[43mN_high\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# random = torch.matmul(G_sqrt,W.reshape(N_high**2,1)).reshape(N_high,N_high)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m z \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m s \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m grad_log_prior \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m s \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m grad_ll \u001b[38;5;241m+\u001b[39m s \u001b[38;5;241m*\u001b[39m W\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train with sampled data\n",
    "epoch_num = 3000\n",
    "lr = 0.001\n",
    "gamma = 0.5\n",
    "\n",
    "K = 2000\n",
    "s = 0.00001\n",
    "step_size = 100\n",
    "minimum_loss = float('inf')\n",
    "loss_track = []\n",
    "\n",
    "# G = ResidualLearning()\n",
    "G = U_net(1,1).to(device)\n",
    "G.apply(weights_init_xavier).to(device)\n",
    "mse = nn.MSELoss(reduction='sum')\n",
    "optG = torch.optim.Adam(G.parameters(), lr = lr, weight_decay=0, betas=(0.5, 0.999))\n",
    "r_scheduleG = torch.optim.lr_scheduler.StepLR(optG, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Logger info\n",
    "dir_name = f'models/model3/16_64_unet/lr{lr}_gamma{gamma}_stepsize{step_size}_K{K}_llsigma_{ll_sigma}_psigma_{prior_sigma}'\n",
    "makedir(dir_name)\n",
    "logger = setup_logging('job0', dir_name, console=True)\n",
    "logger.info(f'Training for {epoch_num} epoches and learning rate is {lr}')\n",
    "\n",
    "for epoch in range(1, epoch_num+1):\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    b_high, low_res = sample_data()\n",
    "    b_high = torch.tensor(b_high).to(torch.float32).to(device)\n",
    "    low_res = torch.tensor(low_res).to(torch.float32).to(device)\n",
    "    \n",
    "    posterior_initial = sample_p_0(low_res)\n",
    "    posterior_final = ula_posterior_preconditioner(posterior_initial, b_high, low_res, G)\n",
    "\n",
    "    optG.zero_grad()\n",
    "    \n",
    "    downscaled = F.interpolate(posterior_final.reshape(1,1,N_high,N_high,N_high),(N_low,N_low,N_low)).reshape(N_low,N_low,N_low)\n",
    "    out = G(downscaled.reshape(1,1,N_low,N_low,N_low)).reshape(N_low,N_low,N_low) + downscaled\n",
    "    loss = mse(out,low_res)\n",
    "\n",
    "    # total_loss += loss\n",
    "    # total_loss /= 8\n",
    "\n",
    "    loss.backward()\n",
    "    optG.step()\n",
    "    \n",
    "    if loss < minimum_loss:\n",
    "        save_model(dir_name, epoch, 'best_model', r_scheduleG, G, optG)\n",
    "        minimum_loss = loss\n",
    "            \n",
    "    if epoch%100 == 0:\n",
    "        save_model(dir_name, epoch, 'model_epoch_{}'.format(epoch), r_scheduleG, G, optG)\n",
    "    \n",
    "    save_model(dir_name, epoch, 'current_epoch', r_scheduleG, G, optG)\n",
    "    loss_track.append(loss.cpu().data.numpy())\n",
    "    np.save(f'{dir_name}/chains/loss_curve.npy', np.array(loss_track))\n",
    "    \n",
    "    print(\"Epoch:\", epoch, \"Loss:\", loss)\n",
    "\n",
    "    r_scheduleG.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upscale By 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_low = 21\n",
    "N_high = 121\n",
    "scale = 6\n",
    "\n",
    "h_low = 1/(N_low-1)\n",
    "x_low = np.arange(0,1.0001,h_low)\n",
    "y_low = np.arange(0,1.0001,h_low)\n",
    "\n",
    "h_high = 1/(N_high-1)\n",
    "x_high = np.arange(0,1.0001,h_high)\n",
    "y_high = np.arange(0,1.0001,h_high)\n",
    "\n",
    "A_high = create_A(N_high)\n",
    "A_low = create_A(N_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Parameters for prior variance\n",
    "prior_sigma = 0.002\n",
    "ll_sigma = 0.006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prior variance\n",
    "G = np.eye(N_high**2) * prior_sigma**2\n",
    "G_inverse = np.eye(N_high**2) * (1/prior_sigma**2)\n",
    "\n",
    "# Turn matrices to tensors\n",
    "G = torch.tensor(G).to(torch.float32).to(device)\n",
    "G_inverse = torch.tensor(G_inverse).to(torch.float32).to(device)\n",
    "A_high = torch.tensor(create_A(N_high)).to(torch.float32).to(device)\n",
    "\n",
    "# Store sparse matrices as sparse tensor\n",
    "A_high = A_high.to_sparse()\n",
    "G = G.to_sparse()\n",
    "G_inverse = G_inverse.to_sparse()\n",
    "operator = torch.spmm(A_high.T,G_inverse).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataFromH5File4(\"/home/pz281@ad.eng.cam.ac.uk/mnt/PhD/Pro_Down_SR/data/21_121_low_forcing.h5\")\n",
    "\n",
    "trainset = random.sample(range(0, 128), 100)\n",
    "testset = [i for i in range(0,128) if i not in trainset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data():\n",
    "    coefficient = random.sample(trainset,1)[0]\n",
    "    forcing = dataset[coefficient][0]\n",
    "    lr = dataset[coefficient][1]\n",
    "    \n",
    "    return forcing, lr\n",
    "\n",
    "\n",
    "def sample_p_0():\n",
    "    # Randomly sampling for initialisation of the Langevin dynamics\n",
    "    # prior = torch.randn(*[batch_size,1,20,20]).to(device)\n",
    "    \n",
    "    # Set the u_low_mean to the initialisation of the Langevin dynamics\n",
    "    posterior_initial = torch.randn([N_high,N_high]).to(torch.float32)\n",
    "    posterior_initial = torch.tensor(posterior_initial).to(device).to(torch.float32)\n",
    "    \n",
    "    return posterior_initial\n",
    "\n",
    "    \n",
    "def ula_posterior_preconditioner(z, b_high, x, G):\n",
    "    \"\"\"\n",
    "    Langevin dynamics with preconditioner\n",
    "    \"\"\"\n",
    "    z = z.clone().detach().requires_grad_(True)\n",
    "    for i in range(K):\n",
    "        # Grad log-likelihood\n",
    "        downscaled = F.interpolate(z.reshape(1,1,N_high,N_high),(N_low,N_low)).reshape(N_low,N_low)\n",
    "        x_hat = downscaled + G(downscaled.reshape(1,N_low,N_low)).reshape(N_low,N_low)\n",
    "        log_likelihood = (-1/(2*math.pow(ll_sigma, 2)) * torch.matmul((x-x_hat).reshape(1,N_low**2),(x-x_hat).reshape(N_low**2,1)))\n",
    "        grad_ll = torch.autograd.grad(log_likelihood, z)[0]\n",
    "\n",
    "        # Grad prior\n",
    "        difference = torch.spmm(A_high,z.reshape(N_high*N_high,1)) - b_high.reshape(N_high**2,1)\n",
    "        # log_prior = - 0.5 * difference.T @ G_inverse @ difference\n",
    "        # grad_log_prior = torch.autograd.grad(log_prior, z)[0]\n",
    "        grad_log_prior = (- torch.spmm(operator,difference)).reshape(N_high,N_high)\n",
    "        \n",
    "        # Random noise term\n",
    "        W = torch.randn(*[N_high,N_high]).to(device)\n",
    "        # random = torch.matmul(G_sqrt,W.reshape(N_high**2,1)).reshape(N_high,N_high)\n",
    "        \n",
    "        z = z + 0.5 * s ** 2 * grad_log_prior + 0.5 * s ** 2 * grad_ll + s * W\n",
    "        # chains_evolution.append(z.cpu().data.numpy())   \n",
    "           \n",
    "    return z.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:45:19,851 : Training for 1000 epoches and learning rate is 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12620/2123801182.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  posterior_initial = torch.tensor(posterior_initial).to(device).to(torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: tensor(4.8323, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 2 Loss: tensor(1.6451, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 3 Loss: tensor(5.1077, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 4 Loss: tensor(2.2748, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 5 Loss: tensor(0.8231, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 6 Loss: tensor(2.1439, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 7 Loss: tensor(0.5367, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 8 Loss: tensor(0.9620, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 9 Loss: tensor(5.6040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 10 Loss: tensor(2.0077, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 11 Loss: tensor(15.0155, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 12 Loss: tensor(0.7064, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 13 Loss: tensor(0.5590, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 14 Loss: tensor(7.4551, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m low_res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(low_res)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m posterior_initial \u001b[38;5;241m=\u001b[39m sample_p_0()\n\u001b[0;32m---> 31\u001b[0m posterior_final \u001b[38;5;241m=\u001b[39m \u001b[43mula_posterior_preconditioner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposterior_initial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_high\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optG\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m downscaled \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(posterior_final\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,N_high,N_high),(N_low,N_low))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,N_low,N_low)\n",
      "Cell \u001b[0;32mIn[34], line 42\u001b[0m, in \u001b[0;36mula_posterior_preconditioner\u001b[0;34m(z, b_high, x, G)\u001b[0m\n\u001b[1;32m     39\u001b[0m     W \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m*\u001b[39m[N_high,N_high])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# random = torch.matmul(G_sqrt,W.reshape(N_high**2,1)).reshape(N_high,N_high)\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrad_log_prior\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m s \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m grad_ll \u001b[38;5;241m+\u001b[39m s \u001b[38;5;241m*\u001b[39m W\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# chains_evolution.append(z.cpu().data.numpy())   \u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z\u001b[38;5;241m.\u001b[39mdetach()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train with sampled data\n",
    "epoch_num = 1000\n",
    "lr = 0.005\n",
    "gamma = 0.5\n",
    "step_size = 30\n",
    "minimum_loss = float('inf')\n",
    "loss_track = []\n",
    "\n",
    "K = 10000\n",
    "s = 0.0004\n",
    "\n",
    "G = ResidualLearning()\n",
    "G.apply(weights_init_xavier).to(device)\n",
    "mse = nn.MSELoss(reduction='sum')\n",
    "optG = torch.optim.Adam(G.parameters(), lr = lr, weight_decay=0, betas=(0.5, 0.999))\n",
    "r_scheduleG = torch.optim.lr_scheduler.StepLR(optG, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Logger info\n",
    "dir_name = f'models/model3/21_121/lr{lr}_gamma{gamma}_stepsize{step_size}_K{K}'\n",
    "makedir(dir_name)\n",
    "logger = setup_logging('job0', dir_name, console=True)\n",
    "logger.info(f'Training for {epoch_num} epoches and learning rate is {lr}')\n",
    "\n",
    "for epoch in range(1, epoch_num+1):\n",
    "    \n",
    "    b_high, low_res = sample_data()\n",
    "    b_high = torch.tensor(b_high).to(torch.float32).to(device)\n",
    "    low_res = torch.tensor(low_res).to(torch.float32).to(device)\n",
    "    \n",
    "    posterior_initial = sample_p_0()\n",
    "    posterior_final = ula_posterior_preconditioner(posterior_initial, b_high, low_res, G)\n",
    "\n",
    "    optG.zero_grad()\n",
    "    \n",
    "    downscaled = F.interpolate(posterior_final.reshape(1,1,N_high,N_high),(N_low,N_low)).reshape(1,N_low,N_low)\n",
    "    out = G(downscaled.reshape(1,N_low,N_low)) + downscaled\n",
    "    loss = mse(out,low_res.reshape(1,N_low,N_low))\n",
    "        \n",
    "    loss.backward()\n",
    "    optG.step()\n",
    "    \n",
    "    if loss < minimum_loss:\n",
    "        save_model(dir_name, epoch, 'best_model', r_scheduleG, G, optG)\n",
    "        minimum_loss = loss\n",
    "            \n",
    "    if epoch%100 == 0:\n",
    "        save_model(dir_name, epoch, 'model_epoch_{}'.format(epoch), r_scheduleG, G, optG)\n",
    "        \n",
    "    save_model(dir_name, epoch, 'current_epoch', r_scheduleG, G, optG)\n",
    "        \n",
    "    loss_track.append(loss.cpu().data.numpy())\n",
    "    np.save(f'{dir_name}/chains/loss_curve.npy', np.array(loss_track))\n",
    "    \n",
    "    print(\"Epoch:\", epoch, \"Loss:\", loss)\n",
    "\n",
    "    r_scheduleG.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
