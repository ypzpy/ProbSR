{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS']='2'\n",
    "os.environ['LD_LIBRARY_PATH']=''\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pengyu.zhang/project/superres/ProbSR/Experiment3\n"
     ]
    }
   ],
   "source": [
    "%cd /home/pengyu.zhang/project/superres/ProbSR/Experiment3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generation import *\n",
    "from scipy.linalg import sqrtm\n",
    "from downscaling import *\n",
    "from utils import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pengyu.zhang/project/superres/ProbSR/Experiment3/Bicubic_Downsampling\n"
     ]
    }
   ],
   "source": [
    "%cd Bicubic_Downsampling/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langevin & Training Downscale Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upscale By 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_low = 15\n",
    "N_high = 60\n",
    "scale = 4\n",
    "a, b, c, d = 1,1,1,0\n",
    "\n",
    "h_low = 1/(N_low-1)\n",
    "x_low = np.arange(0,1.0001,h_low)\n",
    "y_low = np.arange(0,1.0001,h_low)\n",
    "z_low = np.arange(0,1.0001,h_low)\n",
    "\n",
    "h_high = 1/(N_high-1)\n",
    "x_high = np.arange(0,1.0001,h_high)\n",
    "y_high = np.arange(0,1.0001,h_high)\n",
    "z_high = np.arange(0,1.0001,h_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_high = create_A(N_high)\n",
    "A_low = create_A(N_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Parameters for prior variance\n",
    "prior_sigma = 0.0001\n",
    "ll_sigma = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch_tensor(A):\n",
    "    coo = A.tocoo()\n",
    "    values = coo.data\n",
    "    indices = np.vstack((coo.row,coo.col))\n",
    "\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = coo.shape\n",
    "\n",
    "    sparse_tensor = torch.sparse.FloatTensor(i,v,torch.Size(shape))\n",
    "\n",
    "    return sparse_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1322156/2037511837.py:10: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:621.)\n",
      "  sparse_tensor = torch.sparse.FloatTensor(i,v,torch.Size(shape))\n"
     ]
    }
   ],
   "source": [
    "A_high = to_torch_tensor(A_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_high = A_high.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "operator = (A_high.T) * (1/prior_sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataFromH5File4(\"/home/pengyu.zhang/project/superres/ProbSR/Experiment3/data/15_60_low_forcing.h5\")\n",
    "trainset = random.sample(range(0, 1000), 800)\n",
    "testset = [i for i in range(0,1000) if i not in trainset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data():\n",
    "    coefficient = random.sample(trainset,1)[0]\n",
    "    forcing = dataset[coefficient][0]\n",
    "    lr = dataset[coefficient][1]\n",
    "    \n",
    "    return forcing, lr\n",
    "\n",
    "\n",
    "def sample_p_0(x):\n",
    "    # Randomly sampling for initialisation of the Langevin dynamics\n",
    "    # prior = torch.randn(*[batch_size,1,20,20]).to(device)\n",
    "    \n",
    "    # Set the u_low_mean to the initialisation of the Langevin dynamics\n",
    "    # posterior_initial = torch.randn([N_high,N_high]).to(torch.float32)\n",
    "    # posterior_initial = torch.tensor(posterior_initial).to(device).to(torch.float32)\n",
    "    posterior_initial  = F.interpolate(x.reshape(1,1,N_low,N_low,N_low),(N_high,N_high,N_high),mode='trilinear').reshape(N_high,N_high,N_high)\n",
    "    \n",
    "    return posterior_initial\n",
    "\n",
    "    \n",
    "def ula_posterior_preconditioner(z, b_high, x, G):\n",
    "    \"\"\"\n",
    "    Langevin dynamics with preconditioner\n",
    "    \"\"\"\n",
    "    z = z.clone().detach().requires_grad_(True)\n",
    "    sum = 0\n",
    "    for i in range(K):\n",
    "        # Grad log-likelihood\n",
    "        x_hat = G(z.reshape(1,N_high,N_high,N_high)).reshape(N_low,N_low,N_low)\n",
    "        log_likelihood = (-1/(2*math.pow(ll_sigma, 2)) * torch.matmul((x-x_hat).reshape(1,N_low**3),(x-x_hat).reshape(N_low**3,1)))\n",
    "        grad_ll = torch.autograd.grad(log_likelihood, z)[0]\n",
    "\n",
    "        # Grad prior\n",
    "        difference = torch.spmm(A_high,z.reshape(N_high**3,1)) - b_high.reshape(N_high**3,1)\n",
    "        # log_prior = - 0.5 * difference.T @ G_inverse @ difference\n",
    "        # grad_log_prior = torch.autograd.grad(log_prior, z)[0]\n",
    "        grad_log_prior = (- torch.spmm(operator,difference)).reshape(N_high,N_high,N_high)\n",
    "        \n",
    "        # Random noise term\n",
    "        W = torch.randn(*[N_high,N_high,N_high]).to(device)\n",
    "        \n",
    "        z = z + 0.5 * s ** 2 * grad_log_prior + 0.5 * s ** 2 * grad_ll + s * W\n",
    "        if i >= K-10:\n",
    "            sum += z\n",
    "        \n",
    "    sum /= 10\n",
    "           \n",
    "    return sum.detach() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-14 15:50:10,919 : Training for 1000 epoches and learning rate is 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: tensor(5681.1235, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 2 Loss: tensor(956.9529, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 3 Loss: tensor(1934.0833, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 4 Loss: tensor(5.2695, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 5 Loss: tensor(9.6794, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 6 Loss: tensor(5723.1582, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 7 Loss: tensor(226.8138, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 8 Loss: tensor(163.3855, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 9 Loss: tensor(70.2676, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 10 Loss: tensor(816.3237, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 11 Loss: tensor(321.0553, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 12 Loss: tensor(2660.5457, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 13 Loss: tensor(6.3063, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 14 Loss: tensor(1946.9390, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 15 Loss: tensor(513.0198, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 16 Loss: tensor(57.0975, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 17 Loss: tensor(1587.8088, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 18 Loss: tensor(365.2486, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 19 Loss: tensor(3200.9951, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 20 Loss: tensor(170.9767, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 21 Loss: tensor(150.7778, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 22 Loss: tensor(4165.1895, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 23 Loss: tensor(127.9594, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 24 Loss: tensor(1215.1261, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 25 Loss: tensor(28.1738, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 26 Loss: tensor(1168.4119, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 27 Loss: tensor(646.7068, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 28 Loss: tensor(57.4255, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 29 Loss: tensor(412.6198, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 30 Loss: tensor(1107.9323, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 31 Loss: tensor(74.4792, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 32 Loss: tensor(90.1793, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 33 Loss: tensor(5166.4414, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 34 Loss: tensor(911.1285, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 35 Loss: tensor(24.0583, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 36 Loss: tensor(163.6673, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 37 Loss: tensor(62.8022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 38 Loss: tensor(1862.3260, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 39 Loss: tensor(196.8234, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 40 Loss: tensor(778.9346, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 41 Loss: tensor(420.2466, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 42 Loss: tensor(307.5261, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 43 Loss: tensor(217.5392, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 44 Loss: tensor(314.6712, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 45 Loss: tensor(6.0923, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 46 Loss: tensor(314.3267, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 47 Loss: tensor(38.4135, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 48 Loss: tensor(65.6517, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 49 Loss: tensor(994.3890, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 50 Loss: tensor(749.8473, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 51 Loss: tensor(339.5626, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 52 Loss: tensor(17.6389, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 53 Loss: tensor(119.9834, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 54 Loss: tensor(297.8906, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 55 Loss: tensor(71.2496, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 56 Loss: tensor(7.0738, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 57 Loss: tensor(147.2467, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 58 Loss: tensor(268.1885, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 59 Loss: tensor(67.1639, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 60 Loss: tensor(149.8319, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 61 Loss: tensor(286.6989, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 62 Loss: tensor(21.5885, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 63 Loss: tensor(241.5914, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 64 Loss: tensor(173.5639, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 65 Loss: tensor(15.7616, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 66 Loss: tensor(9.4315, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 67 Loss: tensor(12.6099, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 68 Loss: tensor(249.7496, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 69 Loss: tensor(39.9583, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 70 Loss: tensor(197.1521, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 71 Loss: tensor(193.4224, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 72 Loss: tensor(9.5735, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 73 Loss: tensor(15.0223, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 74 Loss: tensor(148.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 75 Loss: tensor(21.3951, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 76 Loss: tensor(206.1055, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 77 Loss: tensor(130.2137, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 78 Loss: tensor(136.5498, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 79 Loss: tensor(272.7480, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 80 Loss: tensor(231.8078, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 81 Loss: tensor(66.3654, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 82 Loss: tensor(77.9611, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 83 Loss: tensor(10.5810, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 84 Loss: tensor(153.0840, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 85 Loss: tensor(11.6145, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 86 Loss: tensor(16.2533, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 87 Loss: tensor(101.4563, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 88 Loss: tensor(10.6853, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 89 Loss: tensor(34.3813, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 90 Loss: tensor(111.7920, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 91 Loss: tensor(180.3033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 92 Loss: tensor(6.7307, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 93 Loss: tensor(37.1217, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 94 Loss: tensor(3.0169, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 95 Loss: tensor(65.1255, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 96 Loss: tensor(21.5757, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 97 Loss: tensor(198.1246, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 98 Loss: tensor(14.3645, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 99 Loss: tensor(27.0948, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 100 Loss: tensor(173.2853, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 101 Loss: tensor(183.7746, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 102 Loss: tensor(28.9139, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 103 Loss: tensor(12.2223, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 104 Loss: tensor(96.8143, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 105 Loss: tensor(163.2896, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 106 Loss: tensor(11.7739, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 107 Loss: tensor(73.1113, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 108 Loss: tensor(13.3120, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 109 Loss: tensor(115.3379, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 110 Loss: tensor(182.3783, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 111 Loss: tensor(32.6804, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 112 Loss: tensor(5.1735, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 113 Loss: tensor(22.9910, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 114 Loss: tensor(196.5175, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 115 Loss: tensor(160.8552, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 116 Loss: tensor(157.9326, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 117 Loss: tensor(171.0088, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 118 Loss: tensor(115.9308, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 119 Loss: tensor(49.1422, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 120 Loss: tensor(16.5373, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 121 Loss: tensor(68.9588, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 122 Loss: tensor(18.0480, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 123 Loss: tensor(2.2119, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 124 Loss: tensor(67.1652, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 125 Loss: tensor(30.6802, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 126 Loss: tensor(100.9347, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 127 Loss: tensor(8.0275, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 128 Loss: tensor(122.8789, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 129 Loss: tensor(9.9247, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 130 Loss: tensor(135.4446, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 131 Loss: tensor(8.9184, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 132 Loss: tensor(5.0224, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 133 Loss: tensor(3.5189, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 134 Loss: tensor(22.4133, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 135 Loss: tensor(146.2805, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 136 Loss: tensor(44.3045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 137 Loss: tensor(131.7129, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 138 Loss: tensor(157.2944, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 139 Loss: tensor(2.1064, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 140 Loss: tensor(97.4053, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 141 Loss: tensor(83.8607, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 142 Loss: tensor(149.2951, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 143 Loss: tensor(96.5692, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 144 Loss: tensor(99.8773, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 145 Loss: tensor(124.9193, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 146 Loss: tensor(38.0075, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 147 Loss: tensor(85.3735, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 148 Loss: tensor(1.4259, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 149 Loss: tensor(60.9122, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 150 Loss: tensor(2.7715, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 151 Loss: tensor(20.0374, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 152 Loss: tensor(54.7906, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 153 Loss: tensor(94.0148, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 154 Loss: tensor(58.1982, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 155 Loss: tensor(23.8658, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 156 Loss: tensor(98.8445, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 157 Loss: tensor(8.7086, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 158 Loss: tensor(7.3717, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 159 Loss: tensor(66.7352, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 160 Loss: tensor(11.4038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 161 Loss: tensor(20.1065, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 162 Loss: tensor(53.6969, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 163 Loss: tensor(27.7166, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 164 Loss: tensor(5.8352, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 165 Loss: tensor(14.1412, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 166 Loss: tensor(4.5249, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 167 Loss: tensor(51.8574, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 168 Loss: tensor(43.6127, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 169 Loss: tensor(127.7075, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 170 Loss: tensor(109.0145, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 171 Loss: tensor(103.5629, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 172 Loss: tensor(32.8905, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 173 Loss: tensor(3.1640, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 174 Loss: tensor(27.8826, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 175 Loss: tensor(38.0235, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 176 Loss: tensor(34.9546, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 177 Loss: tensor(14.7223, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 178 Loss: tensor(28.6390, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 179 Loss: tensor(79.9848, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 180 Loss: tensor(52.2151, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 181 Loss: tensor(1.5818, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 182 Loss: tensor(116.2977, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 183 Loss: tensor(13.0171, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 184 Loss: tensor(17.5065, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 185 Loss: tensor(33.3241, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 186 Loss: tensor(16.9455, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 187 Loss: tensor(31.2172, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 188 Loss: tensor(2.3704, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 189 Loss: tensor(13.0421, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 190 Loss: tensor(108.4930, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 191 Loss: tensor(9.5376, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 192 Loss: tensor(36.8716, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 193 Loss: tensor(68.7188, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 194 Loss: tensor(68.2141, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 195 Loss: tensor(39.3930, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 196 Loss: tensor(41.7802, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 197 Loss: tensor(4.2440, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 198 Loss: tensor(114.4515, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 199 Loss: tensor(1.5663, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 200 Loss: tensor(17.5750, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 201 Loss: tensor(49.2746, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 202 Loss: tensor(3.7192, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 203 Loss: tensor(98.3798, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 204 Loss: tensor(2.4263, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 205 Loss: tensor(37.9009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 206 Loss: tensor(1.5528, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 207 Loss: tensor(23.3869, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 208 Loss: tensor(16.7559, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 209 Loss: tensor(1.5831, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 210 Loss: tensor(13.6149, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 211 Loss: tensor(28.2090, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 212 Loss: tensor(1.2390, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 213 Loss: tensor(1.9240, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 214 Loss: tensor(33.1538, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 215 Loss: tensor(14.8255, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 216 Loss: tensor(1.3007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 217 Loss: tensor(93.7416, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 218 Loss: tensor(19.6873, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 219 Loss: tensor(23.8341, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 220 Loss: tensor(16.7417, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 221 Loss: tensor(13.3580, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 222 Loss: tensor(33.5601, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 223 Loss: tensor(58.4617, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 224 Loss: tensor(4.8988, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 225 Loss: tensor(27.1461, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 226 Loss: tensor(43.5167, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 227 Loss: tensor(77.6010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 228 Loss: tensor(97.1894, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 229 Loss: tensor(3.9880, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 230 Loss: tensor(11.6511, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 231 Loss: tensor(91.9563, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 232 Loss: tensor(1.5851, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 233 Loss: tensor(95.4896, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 234 Loss: tensor(6.5650, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 235 Loss: tensor(52.2209, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 236 Loss: tensor(12.9452, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 237 Loss: tensor(10.1436, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 238 Loss: tensor(85.0397, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 239 Loss: tensor(1.9841, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 240 Loss: tensor(33.6993, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 241 Loss: tensor(44.5079, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 242 Loss: tensor(20.0845, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 243 Loss: tensor(2.1860, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 244 Loss: tensor(1.9966, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 245 Loss: tensor(2.5413, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 246 Loss: tensor(1.8447, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 247 Loss: tensor(7.7125, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 248 Loss: tensor(12.8273, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 249 Loss: tensor(5.8588, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 250 Loss: tensor(13.4218, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 251 Loss: tensor(29.6053, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 252 Loss: tensor(2.2541, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 253 Loss: tensor(39.6367, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 254 Loss: tensor(65.4557, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 255 Loss: tensor(5.9454, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 256 Loss: tensor(14.8774, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 257 Loss: tensor(25.7400, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 258 Loss: tensor(3.4885, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 259 Loss: tensor(91.9422, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 260 Loss: tensor(7.8896, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 261 Loss: tensor(10.2786, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 262 Loss: tensor(93.7571, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 263 Loss: tensor(32.1681, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 264 Loss: tensor(87.2521, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 265 Loss: tensor(13.3663, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 266 Loss: tensor(46.2132, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 267 Loss: tensor(12.4605, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 268 Loss: tensor(1.8321, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 269 Loss: tensor(10.9521, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 270 Loss: tensor(42.1219, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 271 Loss: tensor(6.4556, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 272 Loss: tensor(26.1605, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 273 Loss: tensor(1.8294, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 274 Loss: tensor(3.3828, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 275 Loss: tensor(19.3815, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 276 Loss: tensor(89.9246, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 277 Loss: tensor(2.2015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 278 Loss: tensor(15.9722, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 279 Loss: tensor(94.5034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 280 Loss: tensor(39.9025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 281 Loss: tensor(2.3935, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 282 Loss: tensor(16.2837, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 283 Loss: tensor(7.5482, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 284 Loss: tensor(84.9085, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 285 Loss: tensor(66.5533, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 286 Loss: tensor(6.2370, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 287 Loss: tensor(84.2350, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 288 Loss: tensor(36.3551, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 289 Loss: tensor(95.2126, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 290 Loss: tensor(97.9051, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 291 Loss: tensor(2.3085, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 292 Loss: tensor(2.0610, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 293 Loss: tensor(32.0589, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 294 Loss: tensor(21.7257, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 295 Loss: tensor(16.9137, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 296 Loss: tensor(55.7890, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 297 Loss: tensor(50.1967, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 298 Loss: tensor(37.6500, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 299 Loss: tensor(31.4794, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 300 Loss: tensor(1.1671, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 301 Loss: tensor(6.5042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 302 Loss: tensor(1.1601, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 303 Loss: tensor(1.2788, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 304 Loss: tensor(14.6781, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 305 Loss: tensor(26.2244, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 306 Loss: tensor(67.8789, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 307 Loss: tensor(93.6149, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 308 Loss: tensor(21.5629, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 309 Loss: tensor(68.2115, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 310 Loss: tensor(47.5947, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 311 Loss: tensor(5.4339, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 312 Loss: tensor(74.2617, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 313 Loss: tensor(65.8963, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 314 Loss: tensor(3.5553, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 315 Loss: tensor(0.8407, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 316 Loss: tensor(26.9700, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 317 Loss: tensor(40.3929, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 318 Loss: tensor(30.4488, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 319 Loss: tensor(15.4287, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 320 Loss: tensor(10.8269, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 321 Loss: tensor(50.0076, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 322 Loss: tensor(72.5821, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 323 Loss: tensor(1.8369, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 324 Loss: tensor(7.8367, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 325 Loss: tensor(63.9691, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 326 Loss: tensor(51.0331, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 327 Loss: tensor(97.1513, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 328 Loss: tensor(36.5536, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 329 Loss: tensor(6.4160, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 330 Loss: tensor(88.5706, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 331 Loss: tensor(6.3959, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 332 Loss: tensor(98.0543, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 333 Loss: tensor(4.9826, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 334 Loss: tensor(48.4646, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 335 Loss: tensor(66.2992, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 336 Loss: tensor(8.5442, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 337 Loss: tensor(10.4936, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 338 Loss: tensor(50.6659, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 339 Loss: tensor(2.8232, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 340 Loss: tensor(63.1351, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 341 Loss: tensor(15.3249, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 342 Loss: tensor(21.4485, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 343 Loss: tensor(0.7880, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 344 Loss: tensor(9.7702, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 345 Loss: tensor(8.8994, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 346 Loss: tensor(31.3612, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 347 Loss: tensor(20.4935, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 348 Loss: tensor(2.4295, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 349 Loss: tensor(12.5718, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 350 Loss: tensor(1.2750, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 351 Loss: tensor(1.5850, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 352 Loss: tensor(8.6135, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 353 Loss: tensor(5.1234, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 354 Loss: tensor(3.8115, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 355 Loss: tensor(28.3020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 356 Loss: tensor(40.9707, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 357 Loss: tensor(4.0823, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 358 Loss: tensor(15.0526, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 359 Loss: tensor(94.8471, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 360 Loss: tensor(74.5415, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 361 Loss: tensor(63.1640, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 362 Loss: tensor(94.8447, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 363 Loss: tensor(51.0199, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 364 Loss: tensor(8.0320, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 365 Loss: tensor(89.9205, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 366 Loss: tensor(49.8344, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 367 Loss: tensor(1.8517, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 368 Loss: tensor(18.4883, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Train with sampled data\n",
    "epoch_num = 1000\n",
    "lr = 0.001\n",
    "gamma = 0.5\n",
    "step_size = 50\n",
    "minimum_loss = float('inf')\n",
    "loss_track = []\n",
    "\n",
    "K = 2000\n",
    "s = 0.00001\n",
    "\n",
    "G = DownScale3D(1,16)\n",
    "G.apply(weights_init_xavier).to(device)\n",
    "mse = nn.MSELoss(reduction='sum')\n",
    "optG = torch.optim.Adam(G.parameters(), lr = lr, weight_decay=0, betas=(0.5, 0.999))\n",
    "r_scheduleG = torch.optim.lr_scheduler.StepLR(optG, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Logger info\n",
    "dir_name = f'models/model1/15_60/lr{lr}_gamma{gamma}_stepsize{step_size}_K{K}_llsigma_{ll_sigma}_psigma{prior_sigma}'\n",
    "makedir(dir_name)\n",
    "logger = setup_logging('job0', dir_name, console=True)\n",
    "logger.info(f'Training for {epoch_num} epoches and learning rate is {lr}')\n",
    "\n",
    "for epoch in range(1, epoch_num+1):\n",
    "    \n",
    "    b_high, low_res = sample_data()\n",
    "    b_high = torch.tensor(b_high).to(torch.float32).to(device)\n",
    "    low_res = torch.tensor(low_res).to(torch.float32).to(device)\n",
    "    \n",
    "    posterior_initial = sample_p_0(low_res)\n",
    "    posterior_final = ula_posterior_preconditioner(posterior_initial, b_high, low_res, G)\n",
    "\n",
    "    optG.zero_grad()\n",
    "    \n",
    "    out = G(posterior_final.reshape(1,N_high,N_high,N_high)).reshape(N_low,N_low,N_low)\n",
    "    loss = mse(out,low_res)\n",
    "    \n",
    "    loss.backward()\n",
    "    optG.step()\n",
    "    \n",
    "    if loss < minimum_loss:\n",
    "        save_model(dir_name, epoch, 'best_model', r_scheduleG, G, optG)\n",
    "        minimum_loss = loss\n",
    "            \n",
    "    if epoch%100 == 0:\n",
    "        save_model(dir_name, epoch, 'model_epoch_{}'.format(epoch), r_scheduleG, G, optG)\n",
    "    \n",
    "    save_model(dir_name, epoch, 'current_epoch', r_scheduleG, G, optG)\n",
    "    loss_track.append(loss.cpu().data.numpy())\n",
    "    np.save(f'{dir_name}/chains/loss_curve.npy', np.array(loss_track))\n",
    "    \n",
    "    print(\"Epoch:\", epoch, \"Loss:\", loss)\n",
    "\n",
    "    r_scheduleG.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
